{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the LINC Brain Documentation","text":"<p>These docs reflect a work-in-progress. They may be updated, and potentially incorrect at certain points</p> <p>This documentation is provided to organize the process of cloning the DANDI platform. Get Started Here</p> <p>For more information on DANDI, see here</p>"},{"location":"01_introduction/","title":"Introduction","text":""},{"location":"01_introduction/#what-is-dandi","title":"What is DANDI?","text":"<p>DANDI is:</p> <ul> <li>An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments.</li> <li>A persistent, versioned, and growing collection of standardized datasets.</li> <li>A place to house data to collaborate across research sites.</li> <li>Supported by the BRAIN Initiative and the AWS Public dataset programs.</li> </ul> <p>DANDI provides significant benefits:</p> <ul> <li>A FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data.</li> <li>Rich metadata to support search across data.</li> <li>Consistent and transparent data standards to simplify data reuse and software development. We use the Neurodata Without Borders,  Brain Imaging Data Structure, Neuroimaging Data Model (NIDM), and other BRAIN Initiative standards to organize and search the data. See Data Standards for more information.</li> <li>The data can be accessed programmatically allowing for software to work directly with data in the cloud.</li> <li>The infrastructure is built on a software stack of open source products, thus enriching the ecosystem.</li> </ul>"},{"location":"01_introduction/#properties-of-dandi","title":"Properties of DANDI","text":"<p>Data identifiers: The archive provides persistent identifiers for versioned datasets and assets, thus improving reproducibility of neurophysiology research.</p> <p>Data storage: Cloud-based platform on AWS. Data are available from a public S3 bucket. Data from embargoed datasets are available from a private bucket to owners only.</p> <p>Type of data The archive accepts cellular neurophysiology data including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments and other associated data (e.g. participant information, MRI or other modalities).</p> <p>Accepted Standards and Data File Formats: NWB (HDF5), BIDS (NIfTI, JSON, PNG, TIF, OME.TIF, OME.BTF, OME.ZARR)  (see Data Standards for more details)</p>"},{"location":"01_introduction/#neurophysiology-informatics-challenges-and-dandi-solutions","title":"Neurophysiology Informatics Challenges and DANDI Solutions","text":"Challenges Solutions Most raw data stays in laboratories. DANDI provides a public archive for dissemination of raw and derived data. Non-standardized datasets lead to significant resource needs to understand and adapt code to these datasets. DANDI standardizes all data using NWB and BIDS standards. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. The DANDI ecosystem provides tools for converting data from different instruments into NWB and BIDS. There are many domain general places to house data (e.g. Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant scientific metadata. DANDI is focused on neurophysiology data and related metadata. Datasets are growing larger, requiring compute services to be closer to data. DANDI provides Dandihub, a JupyterHub instance close to the data. Neurotechnology is evolving and requires changes to metadata and data storage. DANDI works with community members to improve data standards and formats. Consolidating and creating robust algorithms (e.g. spike sorting) requires varied data sources. DANDI provides access to many different datasets."},{"location":"100_about_this_doc/","title":"About This Documentation","text":"<p>This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk.</p>"},{"location":"100_about_this_doc/#serving-the-docs-locally","title":"Serving the Docs Locally","text":"<p>This project uses the MkDocs tool with the Material theme and extra plugins to generate the website.</p> <p>To test locally, you will need to install the Python dependencies. To do that, type the following commands:</p> <pre><code>git clone https://github.com/dandi/handbook.git\ncd handbook\npip install -r requirements.txt\n</code></pre> <p>If you are working on your fork, simply replace <code>https://github.com/dandi/handbook.git</code> with <code>git clone git@github.com/&lt;username&gt;/handbook.git</code> , where <code>&lt;username&gt;</code> is your GitHub username.</p> <p>Once done, you need to run MkDocs. Simply type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Finally, open <code>http://127.0.0.1:8000/</code> in your browser, and you should see the default home page of the documentation being displayed.</p>"},{"location":"10_using_dandi/","title":"Using DANDI","text":"<p>DANDI allows you to work with stored neurophysiology data in multiple ways. You can search, view, and download files, all without registering for a DANDI account. As a registered user, you can also create these collections of data  along with metadata and publish them to the DANDI platform. </p>"},{"location":"10_using_dandi/#dandisets","title":"Dandisets","text":"<p>DANDI stores cellular neurophysiology data in Dandisets.</p> <p>A Dandiset is a collection of assets (files and their metadata) and metadata about the collection.</p> <ul> <li>A Dandiset is organized in a structured manner to help users and software tools interact with it.</li> <li>Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g. https://identifiers.org/DANDI:000004). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset.</li> </ul>"},{"location":"10_using_dandi/#quick-start","title":"Quick Start","text":"<p>If you are new to DANDI, all you need is an Internet connection to use the DANDI Web application to view  and download files from a  public  Dandiset.  Registration is not required.</p> <p>To view a specific public Dandiset and download one of its files:</p> <ol> <li> <p>At the top of the DANDI Web application, click PUBLIC DANDISETS to see all Dandisets currently available in the     archive. You can sort them by name, identifier, or date of modification.</p> </li> <li> <p>Search for a specific Dandiset by contributor name, modality, or species.</p> </li> <li> <p>Click a Dandiset to open its landing page and view important information such as contact information,     description, license, access information and keywords, and simple statistics.</p> </li> <li> <p>From the right side of the Dandiset landing page, click FILES to see a list of all folders and files for that     Dandiset. Click the download icon  to download a     specific file.  Note: To download an entire Dandiset, you will need to follow the instructions in the     Download section to install and use the DANDI Python client tool.</p> </li> </ol>"},{"location":"10_using_dandi/#next-steps","title":"Next steps","text":"<p>Although anyone on the Internet  can view and download public Dandisets, registered users can also create Dandisets, upload data, and publish  the Dandiset to generate a DOI for it. See the sections that follow for more detailed information about the DANDI project, as well as instructions on how  to work with public Dandisets or to create and publish you own as a registered user. </p>"},{"location":"10_using_dandi/#dandiset-actions","title":"Dandiset Actions","text":"<p>The DANDI project contains the DANDI Web application, the DANDI Python client tool, and the DANDI JupyterHub  instance. These tools can be used to perform actions on Dandisets. </p> <p></p> <p>You can learn more about the Dandiset actions in separate sections:</p> <ul> <li>View</li> <li>Download</li> <li>Upload</li> <li>Publish</li> </ul>"},{"location":"10_using_dandi/#tools-to-interact-with-dandi","title":"Tools to interact with DANDI","text":""},{"location":"10_using_dandi/#dandi-web-application","title":"DANDI Web application","text":"<p>The DANDI Web application allows you to:</p> <ul> <li>Search across all public Dandisets</li> <li>Download data from public Dandisets</li> <li>Create a new Dandiset and provide metadata</li> <li>Publish your Dandiset</li> </ul>"},{"location":"10_using_dandi/#dandi-python-client","title":"DANDI Python client","text":"<p>The DANDI Python client allows you to:</p> <ul> <li>Download Dandisets and individual subject folders or files</li> <li>Organize your data locally before upload</li> <li>Upload Dandisets</li> </ul> <p>Before you can use the DANDI Python client, you have to install the package with <code>pip install dandi</code> in a Python 3.7+ environment.</p> <p>You should check the Dandi Debugging section in case of any problems.</p>"},{"location":"10_using_dandi/#dandihub-analysis-platform","title":"Dandihub analysis platform","text":"<p>Dandihub provides a JupyterHub instance in the cloud to interact with the data stored in DANDI.</p> <p>To use the hub, you will need to register for an account using the DANDI Web application.  Note that <code>Dandihub</code> is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"10_using_dandi/#citing-dandi","title":"Citing DANDI","text":"<p>You can add the following statement to the methods section of your manuscript.</p> <p>Data and associated metadata were uploaded to the DANDI archive [RRID:SCR_017571] using    the Python command line tool (https://doi.org/10.5281/zenodo.7041535). The data were first    converted into the NWB format (https://doi.org/10.1101/2021.03.13.435173) and  organized    into a BIDS-like (https://doi.org/10.1038/sdata.2016.44) structure.</p> <p>You can refer to DANDI using any of the following options:</p> <ul> <li> <p>Using an RRID RRID:SCR_017571. </p> </li> <li> <p>Using the DANDI CLI reference: https://doi.org/10.5281/zenodo.7041535</p> </li> </ul>"},{"location":"11_view/","title":"Viewing Dandisets","text":""},{"location":"11_view/#browse-dandisets","title":"Browse Dandisets","text":"<p>When you go to the DANDI Web application, you can click on <code>PUBLIC DANDISET</code> to access all Dandisets currently available  in the archive, and you can sort them by name, identifier, size, or date of modification.</p> <p></p>"},{"location":"11_view/#search-dandisets","title":"Search Dandisets","text":"<p>In addition, you can search across the Dandisets for any text part of the Dandiset metadata record.  The text may be about contributor names, modalities, or species.  For example,  <code>\"house mouse\"</code> will return a subset of all Dandisets, while <code>\"mouse house\"</code> will likely not return any. When unquoted each word is used as an <code>OR</code>.</p> <p></p> <p>When you click on one of the Dandisets, you can see that the searching phrase can appear in the description, keywords, or in the assets summary.</p> <p></p>"},{"location":"11_view/#dandisets-metadata","title":"Dandisets Metadata","text":"<p>The landing page of each Dandiset contains important information including  metadata provided by the owners such as contact information, description, license, access information and keywords,   simple statistics for a Dandiset such as size of the Dandiset and number of files, or  a summary of the Dandiset including information about species, techniques, and standards.</p> <p></p> <p>If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources</p> <p>While most of the metadata is summarized on the landing page, some additional information can be  found by clicking <code>Metadata</code> on the right-side panel. For Dandiset owners, this button also allows  adding relevant metadata to populate the landing page.</p> <p></p>"},{"location":"11_view/#file-view","title":"File View","text":"<p>The right side panel allows you also to access a file browser to navigate the list of folders and files in a Dandiset.</p> <p></p> <p>Any file in the Dandiset has a download icon You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. In addition, there is an info icon that leads to full asset metadata. Some files also have a link to external  services that can open the file. Note: that these services often have size limits and hence are activated only for appropriately sized files.</p>"},{"location":"11_view/#my-dandisets","title":"My Dandisets","text":"<p>If you log in as a registered user, you will also see <code>My Dandisets</code> tab:</p> <p></p> <p>By clicking the tab, you can access all the Dandisets you own. For these Dandisets, you can edit and update  metadata through the Dandiset actions section, and add or remove other owners or data.</p>"},{"location":"12_download/","title":"Downloading Data and Dandisets","text":"<p>You can download the content of a Dandiset using the DANDI Web application (such a specific file) or entire  Dandisets using the DANDI Python CLI.</p>"},{"location":"12_download/#using-the-dandi-web-application","title":"Using the DANDI Web Application","text":"<p>Once you have the Dandiset you are interested in (see more in the Dandiset View section), you can download the content of the Dandiset. On the landing page of each Dandiset, you can find <code>Download</code> button on the right-hand panel. After clicking the  button, you will see the specific command you can use with DANDI Python CLI (as well as the information on how to download the CLI).</p> <p></p>"},{"location":"12_download/#download-specific-files","title":"Download specific files","text":"<p>The right-side panel of the Dandiset landing page allows you also to access the list of folders and files.</p> <p></p> <p>Each file in the Dandiset has a download icon next to it, clicking the icon will start the download process.</p>"},{"location":"12_download/#using-the-python-cli-client","title":"Using the Python CLI Client","text":"<p>The DANDI Python client gives you more options, such as downloading entire  Dandisets.</p> <p>Before You Begin: You need to have Python 3.7+ and install the DANDI Python Client using <code>pip install dandi</code>. If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"12_download/#download-a-dandiset","title":"Download a Dandiset","text":"<p>To download an entire Dandiset, you can use the same command as suggested by DANDI web application, e.g.: </p> <p><code>dandi download DANDI:000023</code></p>"},{"location":"12_download/#download-data-for-a-specific-subject-from-a-dandiset","title":"Download data for a specific subject from a Dandiset","text":"<p>You can download data for specific subjects.  Names of the subjects can be found on DANDI web application or by running a command with the DANDI CLI: <code>dandi ls -r  DANDI:000023</code>. Once you have the subject ID, you can download the data, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/_draft_/assets/?path=sub-811677083</code></p> <p>You should replace <code>_draft_</code> with a specific version you are interested in (e.g. <code>0.210914.1900</code> in the case of this Dandiset).</p> <p>You can also use the link from DANDI web application, e.g.:</p> <p><code>dandi download https://dandiarchive.org/dandiset/000023/0.210914.1900/files?location=sub-541516760%2F</code></p>"},{"location":"12_download/#download-a-specific-file-from-a-dandiset","title":"Download a specific file from a Dandiset","text":"<p>You can download a specific file from a Dandiset when the link for the specific file can be found on the DANDI web  application, e.g.:</p> <p><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/</code></p> <p>Hint: <code>dandi download</code> supports a number of resource identifiers to point to a Dandiset, folder, or file.  Providing  an incorrect URL (e.g. <code>dandi download wrongurl</code>) will provide a list of supported identifiers.</p>"},{"location":"135_validation/","title":"Validation Levels for NWB Files","text":"<p>To be accepted by DANDI, NWB files must conform to criteria that are enforced via three levels of validation:</p>"},{"location":"135_validation/#nwb-file-validation","title":"NWB File Validation","text":"<p>PyNWB validation is used to validate the NWB files,  ensuring that they meet the specifications of core NWB and of any NWB extensions that were used. Generally  speaking, all files produced by PyNWB and MatNWB should pass validation, however there are occasional bugs. More  often, NWB files that fail to meet these criteria have been created outside PyNWB and MatNWB.</p>"},{"location":"135_validation/#critical-nwb-checks","title":"Critical NWB Checks","text":"<p>The NWB Inspector scans NWB files using heuristics to find mistakes  or areas for improvements in NWB files. There are three levels of importance for checks: CRITICAL, BEST PRACTICE  VIOLATIONS, and BEST PRACTICE SUGGESTIONS. CRITICAL warnings indicate some internal inconsistency in the data of the  NWB files. The NWB Inspector will print out all warnings, but only CRITICAL warnings will prevent a file from being  uploaded to DANDI. Errors in NWB Inspector will be block upload as well, but reflect a problem with the NWB  Inspector software as opposed to the NWB file. </p>"},{"location":"135_validation/#missing-dandi-metadata","title":"Missing DANDI Metadata","text":"<p>DANDI has requirements for metadata beyond what is strictly required for NWB validation. The following metadata must  be present in the NWB file for a successful upload to DANDI: - You must define a <code>Subject</code> object. - The <code>Subject</code> object must have a <code>subject_id</code> attribute. - The <code>Subject</code> object must have a <code>species</code> attribute. This can either be the Latin binomial, e.g. \"Mus musculus\", or    an NCBI taxonomic identifier. - The <code>Subject</code> object must have a <code>sex</code> attribute. It must be \"M\", \"F\", \"O\" (other), or \"U\" (unknown). - The <code>Subject</code> object must have either <code>date_of_birth</code> or <code>age</code> attribute. It must be in ISO 8601 format, e.g. \"P70D\"    for 70 days, or, if it is a range, must be \"[lower]/[upper]\", e.g. \"P10W/P12W\", which means \"between 10 and 12 weeks\"</p> <p>These requirements are specified in the  DANDI configuration file of NWB Inspector.</p> <p>Passing all of these levels of validation can sometimes be tricky. If you have any questions, please ask them via the  DANDI Help Desk and we would be happy to assist you.</p>"},{"location":"13_upload/","title":"Creating Dandisets and Uploading Data","text":"<p>To create a new Dandiset and upload your data, you need to have a DANDI account.</p>"},{"location":"13_upload/#create-an-account-on-dandi","title":"Create an Account on DANDI","text":"<p>To create a DANDI account:</p> <ol> <li>Create a GitHub account if you don't have one.</li> <li>Using your GitHub account, register a DANDI account.</li> </ol> <p>You will receive an email acknowledging activation of your account within 24 hours, after which you can log in to DANDI using GitHub by clicking the login button.</p>"},{"location":"13_upload/#create-a-dandiset-and-add-data","title":"Create a Dandiset and Add Data","text":"<p>You can create a new Dandiset at http://dandiarchive.org. This Dandiset can be fully  public or embargoed  according to NIH policy. When you create a Dandiset, a permanent ID is automatically assigned to it. To prevent the production server from being inundated with test Dandisets, we encourage developers to develop  against the development server (https://gui-staging.dandiarchive.org/). Note  that the development server should not be used to stage your data. All data are uploaded as draft and can be adjusted before publishing on the production server. The development server is primarily used by users learning to use DANDI or by developers.</p> <p>The below instructions will alert you to where the commands for interacting with these  two different servers differ slightly. </p>"},{"location":"13_upload/#setup","title":"Setup","text":"<ol> <li>Log in to DANDI and copy your API key. Click on your user initials in the     top-right corner after logging in. Production (dandiarchive.org) and staging (gui-staging.dandiarchive.org) servers        have different API keys and different logins.</li> <li> <p>Locally:</p> <ol> <li>Create a Python environment. This is not required, but strongly recommended; e.g. miniconda,      virtualenv.</li> <li> <p>Install the DANDI CLI into your Python environment:</p> <pre><code>pip install -U dandi\n</code></pre> </li> <li> <p>Store your API key somewhere that the CLI can find it; see \"Storing       Access Credentials\" below.</p> </li> </ol> </li> </ol>"},{"location":"13_upload/#data-uploadmanagement-workflow","title":"Data upload/management workflow","text":"<ol> <li>Register a Dandiset to generate an identifier. You will be asked to enter     basic metadata: a name (title) and description (abstract) for your dataset.     Click <code>NEW DANDISET</code> in the Web application (top right corner) after logging in.      After you provide a name and description, the dataset identifier will be created;      we will call this <code>&lt;dataset_id&gt;</code>.</li> <li> <p>NWB format:</p> <ol> <li>Convert your data to NWB 2.1+ in a local folder. Let's call this <code>&lt;source_folder&gt;</code>. We suggest beginning the conversion process using only a small amount of data so that common issues may be spotted earlier in the process. This step can be complex depending on your data. NeuroConv automates conversion to NWB from a variety of popular formats. nwb-overview.readthedocs.io points to more tools helpful for working with NWB files, and BIDS converters if you are preparing a BIDS dataset containing NWB files. Feel free to reach out to us for help.</li> <li> <p>Check your files for NWB Best Practices by installing the NWBInspector (<code>pip install -U nwbinspector</code>) and running</p> <pre><code>    nwbinspector &lt;source_folder&gt; --config dandi\n</code></pre> </li> <li> <p>Thoroughly read the NWBInspector report and try to address as many issues as possible. DANDI will prevent validation and upload of any issues labeled as level 'CRITICAL' or above when using the <code>--config dandi</code> option. See     \"Validation Levels for NWB Files\" for more information about validation criteria for     uploading NWB     files and which are deemed critical. We recommend regularly running the inspector early in the process to generate the best NWB files possible.  Note that some autodetected violations, such as <code>check_data_orientation</code>, may be safely ignored in the event     that the data is confirmed to be in the correct form; this can be done using either the <code>--ignore &lt;name_of_check_to_suppress&gt;</code> flag or a config file. See the NWBInspector CLI documentation for more details and other options, or type <code>nwbinspector --help</code>. If the report is too large to efficiently navigate in your console, you can save a report using</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi --report-file-path &lt;report_location&gt;.txt\n</code></pre> </li> <li> <p>Once your files are confirmed to adhere to the Best Practices, perform an official validation of the NWB files by running: <code>dandi validate --ignore DANDI.NO_DANDISET_FOUND &lt;source_folder&gt;</code>.     If you are having trouble with validation, make sure the conversions were run with the most recent version of <code>dandi</code>, <code>PyNWB</code> and <code>MatNWB</code>.</p> </li> <li>Now, prepare and fully validate again within the dandiset folder used for upload:<pre><code>dandi download https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft\ncd &lt;dataset_id&gt;\ndandi organize &lt;source_folder&gt; -f dry\ndandi organize &lt;source_folder&gt;\ndandi validate .\ndandi upload\n</code></pre> </li> </ol> <p>Note that the <code>organize</code> steps should not be used if you are preparing a BIDS dataset with the NWB files. Uploading to the development server is controlled via <code>-i</code> option, e.g. <code>dandi upload -i dandi-staging</code>. Note that validation is also done during <code>upload</code>, but ensuring compliance using <code>validate</code> prior upload helps avoid interruptions of the lengthier upload process due to validation failures. 6. Add metadata by visiting your Dandiset landing page:    <code>https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft</code> and clicking on the <code>METADATA</code> link.</p> </li> </ol> <p>If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"13_upload/#storing-access-credentials","title":"Storing Access Credentials","text":"<p>By default, the DANDI CLI looks for an API key in the <code>DANDI_API_KEY</code> environment variable.  To set this on Linux or macOS, run</p> <pre><code>export DANDI_API_KEY=personal-key-value\n</code></pre> <p>*Note that there are no spaces around the \"=\".</p> <p>If this is not set, the CLI will look up the API key using the keyring library, which supports numerous backends, including the system keyring, an encrypted keyfile, and a plaintext (unencrypted) keyfile.</p> <ul> <li> <p>You can store your API key where the <code>keyring</code> library can find it by using   the <code>keyring</code> program: Run <code>keyring set dandi-api-dandi key</code> and enter the   API key when asked for the password for <code>key</code> in <code>dandi-api-dandi</code>.</p> </li> <li> <p>You can set the backend the <code>keyring</code> library uses either by setting the   <code>PYTHON_KEYRING_BACKEND</code> environment variable or by filling in the <code>keyring</code>   library's configuration file.   IDs for the available backends can be listed by running <code>keyring --list</code>.  If   no backend is specified in this way, the library will use the available   backend with the highest priority.</p> </li> </ul> <p>If the API key isn't stored in either the <code>DANDI_API_KEY</code> environment variable or in the keyring, the CLI will prompt you to enter the API key, and then it will store it in the keyring.  This may cause you to be prompted further; you may be asked to enter a password to encrypt/decrypt the keyring, or you may be asked by your OS to confirm whether to give the DANDI CLI access to the keyring.</p> <ul> <li>If the DANDI CLI encounters an error while attempting to fetch the API key   from the default keyring backend, it will fall back to using an encrypted   keyfile (the <code>keyrings.alt.file.EncryptedKeyring</code> backend).  If the keyfile   does not already exist, the CLI will ask you for confirmation; if you answer   \"yes,\" the <code>keyring</code> configuration file (if it does not already exist; see   above) will be configured to use <code>EncryptedKeyring</code> as the default backend.   If you answer \"no,\" the CLI will exit with an error, and you must store the   API key somewhere accessible to the CLI on your own.</li> </ul>"},{"location":"14_publish/","title":"Publishing Dandisets","text":"<p>Once you create a Dandiset, DANDI will automatically create a <code>draft</code> version of the Dandiset that  can be  changed as many times as needed. </p> <p>Prior to publishing, you should edit your metadata appropriately (e.g. people and funding contributors, protocol information, keywords, related resources in other  sources such as publications, code repositories, and other data). If you need to change the Dandiset or its metadata in the future, you will need to create another version of your  Dandiset. The content of the specific published version of the Dandiset cannot be changed.</p> <p>When you are ready to publish your data and to create a unique version of your  Dandiset, click the <code>Publish</code> button (on the right panel of the landing page). Note that the <code>Publish</code>  button is only active if all the metadata and asset errors are fixed.</p>"},{"location":"15_debugging/","title":"Debugging","text":"<p>If something goes wrong while using the Python CLI client, the first place to check for more information so that you can file a quality bug report is the logs.  Every command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running.  The location of the logs varies by platform, e.g.:</p> <ul> <li>Linux: <code>~/.cache/dandi-cli/log</code> or <code>$XDG_CACHE_HOME/dandi-cli/log</code></li> <li>macOS: <code>~/Library/Logs/dandi-cli</code></li> </ul> <p>Logs are named with a combination of the time at which the <code>dandi</code> command started running and the process ID of the command.</p> <p>Recent versions of the client include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded.  As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing <code>-l DEBUG</code> or <code>--log-level DEBUG</code> on the command line.  Note that this option goes between the main <code>dandi</code> command and the name of the subcommand:</p> <pre><code># Right:\ndandi -l DEBUG upload\n\n# Wrong:\ndandi upload -l DEBUG\n</code></pre> <p>In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars.  For the <code>delete</code>, <code>organize</code>, <code>upload</code>, and <code>validate</code> commands, this can be done by setting the <code>DANDI_DEVEL</code> environment variable and passing <code>--devel-debug</code> to the command:</p> <pre><code>DANDI_DEVEL=1 dandi upload --devel-debug\n</code></pre> <p>For the <code>download</code> command, the equivalent is the <code>-f debug</code>/<code>--format debug</code> option:</p> <pre><code>dandi download -f debug\n</code></pre> <p>More advanced users who are familiar with the Python debugger can instruct the client to automatically open the debugger if any errors occur by supplying the <code>--pdb</code> option to the command.  Like the <code>-l</code>/<code>--log-level</code> option, the <code>--pdb</code> option must be placed between <code>dandi</code> and the name of the subcommand.</p>"},{"location":"20_project_structure/","title":"Project Structure","text":"<p>The DANDI project can be represented schematically:</p> <p></p> <p>The Client side contains the DANDI Python CLI and DANDI Web application.</p> <p>The Server side contains a RESTful API and DANDI JupyterHub.</p> <p>The Dandiset is a file organization to store data together with metadata.</p> <p>The DANDI project is organized around several GitHub repositories:</p> Repository Description DANDI archive Contains the code for deploying the client-side Web application frontend based on the Vue.js framework as well as a Django-based backend to run the DANDI REST API. DANDI JupyterHub Contains the code for deploying a JupyterHub instance to support interaction with the DANDI archive. DANDI Python client Contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. handbook Provides the contents of this website. helpdesk Contains our community help platform where you can submit issues. schema Provides the details and some supporting code for the DANDI metadata schema. schema Python library Provides a Python library for updating the schema and for creating and validating DANDI objects. website Provides an overview of the DANDI project and the team members and collaborators."},{"location":"30_data_standards/","title":"Data Standards","text":"<p>DANDI requires uploaded data to adhere to community data standards.  These standards help data curators package all the necessary metadata and provide a uniform structure so that data can be more easily understood and reused by future users.  DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: </p> <ul> <li>For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB)</li> <li>For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS)</li> </ul> <p>For microscopy data from immunostaining, we are using the BIDS extension for microscopy.</p> <p>To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, do not hesitate to reach out using the dandi helpdesk and we would be happy to assist.</p>"},{"location":"30_data_standards/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)","text":"<p>NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python (PyNWB) and MATLAB (MatNWB), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats.  The best way to get help from the NWB community is through the NWB user Slack channel.</p>"},{"location":"30_data_standards/#brain-imaging-data-format-bids","title":"Brain Imaging Data Format (BIDS)","text":"<p>BIDS is a way to organize and describe neuroimaging and behavioral data.  See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS.</p> <p>For microscopy and associated MR data, use the BIDS extension for microscopy.</p>"},{"location":"35_data_licenses/","title":"Data Licenses","text":"<p>To create a Dandiset, you must select a license under which to share the data. Because the DANDI Archive provides a platform for open data sharing, the licenses come from Creative Commons, an international nonprofit organization dedicated to establishing, growing, and maintaining a shared commons in the spirit of open source.</p> <p>These licenses enable the dataset's copyright holder to grant permissions to others to share and use the data for a wide range of purposes. The licenses available to users of the Archive are as follows:</p> <ul> <li> <p>Attribution (CC-BY-4.0). This license grants permission to share the data to others and to adapt the data by remixing, transforming, and building upon it, so long as appropriate credit is given to the copyright holder and any changes to the original are clearly indicated. The dataset may be used for any purpose (even commercial ones). Note that this license retains the original copyright while granting permissive access to the data to all others.</p> </li> <li> <p>Public domain dedication (CC0-1.0). This license dedicates the dataset to the public domain, relinquishing copyright and therefore allowing anyone to use the dataset for any purpose without restriction.</p> </li> </ul> <p>You can learn more about the theory of how the Creative Commons licenses operate at their website. If you have any questions or concerns, send a message to help@dandiarchive.org.</p>"},{"location":"40_development/","title":"Developer Notes","text":"<p>This page contains important information for anyone starting development work on the DANDI project.</p>"},{"location":"40_development/#overview","title":"Overview","text":"<p>The DANDI archive dev environment comprises three major pieces of software: <code>dandi-archive</code>, <code>dandi-cli</code>, and <code>dandi-schema</code>.</p>"},{"location":"40_development/#dandi-archive","title":"<code>dandi-archive</code>","text":"<p><code>dandi-archive</code> is the web frontend application; it connects to <code>dandi-api</code> and provides a user interface to all the DANDI functionality.  <code>dandi-archive</code> is a standard web application built with <code>yarn</code>. See the <code>dandi-archive</code> README for instructions on how to build it locally.</p> <p>The Django application makes use of several services to provide essential function for the DANDI REST API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application).</p> <p>The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the Develop with Docker quickstart.</p>"},{"location":"40_development/#dandi-cli","title":"<code>dandi-cli</code>","text":"<p><code>dandi-cli</code> is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to use this tool when developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install <code>dandi-cli</code> with a command like <code>pip install dandi</code> (then invoke <code>dandi</code> on the command line to run the tool), or build it locally following the instructions in the <code>dandi-cli</code> README.</p>"},{"location":"40_development/#dandi-schema","title":"<code>dandi-schema</code>","text":"<p><code>dandi-schema</code> is a Python library for  creating, maintaining, and validating the DANDI metadata models for dandisets  and assets. You may need to make use of this tool when improving models, or  migrating metadata. You can install <code>dandi-schema</code> with a command like  <code>pip install dandi-schema</code>. When releases are published through dandi-schema,  corresponding json-schemas are generated in the release folder of the dandi schema repo. See the <code>dandi-schema</code> README for instructions on  viewing the schemas.</p>"},{"location":"40_development/#technologies-used","title":"Technologies Used","text":"<p>This section details some foundational technologies used in <code>dandi-archive</code>. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well.</p> <p>JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript.</p> <p>Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX.</p> <p>Vuetify. The components make heavy use of the Vuetify component library.</p> <p>Python3. The backend code is written in Python 3.</p> <p>Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation.</p> <p>For general help with <code>dandi-archive</code>, contact @waxlamp.</p>"},{"location":"40_development/#deployment","title":"Deployment","text":"<p>The DANDI project uses automated services to continuously deploy both the <code>dandi-api</code> backend and the <code>dandi-archive</code> frontend.</p> <p>Heroku manages backend deployment automatically from the <code>master</code> branch of the <code>dandi-api</code> repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the <code>dandi-infrastructure</code> repository. If you need access to the Heroku DANDI organization, talk to @satra.</p> <p>Netlify manages the frontend deployment process. Similarly to <code>dandi-api</code>, these deployments are based on the <code>master</code> branch of <code>dandi-archive</code>. The <code>netlify.toml</code> file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra.</p>"},{"location":"40_development/#code-hosting","title":"Code Hosting","text":"<p>All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built.</p> <p>However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews.</p>"},{"location":"40_development/#email-lists","title":"Email Lists","text":"<p>The project's email domain name services are managed via Terraform as AWS Route 53 entries. This allows the API server to send emails to users, etc. It also means we need a way to forward incoming emails to the proper mailing list--this is accomplished with a service called ImprovMX.</p> <p>The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help; both of them are forwarded to dandi@mit.edu, a mailing list containing the leaders and developers of the project. The forwarding is done by the ImprovMX service, and more such email addresses can be created as needed within that service.</p> <p>If you need the credentials for logging into ImprovMX, speak to Roni Choudhury (roni.choudhury@kitware.com).</p>"},{"location":"40_development/#miscellaneous-tips-and-information","title":"Miscellaneous Tips and Information","text":""},{"location":"40_development/#use-email-address-to-log-into-dev-django-admin-panel","title":"Use email address to log into dev Django admin panel","text":"<p>Once <code>dandi-api</code> is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username.</p> <p>One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup.</p>"},{"location":"40_development/#refresh-github-login-to-log-into-prod-django-admin-panel","title":"Refresh GitHub login to log into prod Django admin panel","text":"<p>To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account.</p> <p>However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g. https://api.dandiarchive.org/admin) and you should be logged back in there.</p>"},{"location":"40_development/#why-do-incoming-emails-to-dandiarchiveorg-look-crazy","title":"Why do incoming emails to dandiarchive.org look crazy?","text":"<p>When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general (e.g. messages sent from the API server to users).</p>"},{"location":"40_initialization/","title":"Initialization","text":"<p>LINC Brain relies on a handful of vendor services to operate:</p> <p>The following accounts must be made to create LINC Brain in a production setting:</p> <p>\u2022 Heroku</p> <p>\u2022 AWS</p> <p>\u2022 GitHub</p> <p>\u2022 Terraform Cloud</p> <p>\u2022 Netlify</p> <p>\u2022 Sentry</p> <p>\u2022 PyPI</p> <p>\u2022 Datalad (TBD)</p> <p>\u2022 git-annex (TBD)</p>"},{"location":"40_initialization/#heroku","title":"Heroku","text":""},{"location":"40_initialization/#create-your-own-heroku-account","title":"Create your own Heroku account","text":"<p>No special steps here, just create!</p>"},{"location":"40_initialization/#create-a-team","title":"Create a \"Team\"","text":"<p>Send invites to the appropriate e-mail addresses to onboard your team</p>"},{"location":"40_initialization/#create-an-app","title":"Create an \"App\"","text":"<p>You can name whatever you wish -- no need to add a pipeline, etc. Be sure to note the app name</p> <p> </p> <p>Your app will be an empty template, but that is OK -- more to come here!</p>"},{"location":"40_initialization/#obtain-heroku-api-key","title":"Obtain Heroku API Key","text":"<p>You'll need to provide access to Heroku for GitHub, Terraform, etc. -- thus, you'll need to generate an API Key</p> <p>First go to your <code>Account Settings</code>:</p> <p> </p> <p>Next, find the <code>API Key</code> section and generate a new key.</p> <p> </p> <p>Keep this value for further steps.</p>"},{"location":"40_initialization/#aws","title":"AWS","text":""},{"location":"40_initialization/#create-your-organizations-aws-account","title":"Create your organization's AWS account","text":"<p>No special steps here, just create!</p>"},{"location":"40_initialization/#create-an-iam-user-group-with-administratoraccess","title":"Create an IAM User Group with <code>AdministratorAccess</code>","text":"<p>Ensure that you click the right policy in the <code>Attach permissions policies</code> section.</p> <p> </p> <p>Note: If you know more refined permissions to give the IAM Group, that is preferred, as those with access to the credentials of <code>AdministratorAccess</code> in AWS can be an extreme security hazard if not managed appropriately</p> <p> </p>"},{"location":"40_initialization/#create-an-iam-user-that-lives-in-the-user-group-you-made","title":"Create an IAM user that lives in the User Group you made","text":"<p>Next, give a unique name -- no need to enable Console Access</p> <p> </p> <p>Lastly, add them to the Group you made in the step above:</p> <p> </p>"},{"location":"40_initialization/#create-security-credentials","title":"Create security credentials","text":"<p>You'll need to create <code>Security Credentials</code> for your User -- these credentials will be specifically used in your <code>Terraform Cloud</code> setup</p> <p>Firstly, go to your User and click the <code>Security Credentials</code> tab:</p> <p> </p> <p>Navigate to <code>Create access key</code></p> <p> </p> <p>You'll be prompted to provide a reason for the access key creation. It doesn't matter much, so <code>Other</code> is a completely acceptable choice</p> <p> </p> <p>Time to create the key!</p> <p> </p> <p>You'll be provided with the values of your <code>Access key</code> and <code>Secret access key</code> -- store these values somewhere secure and accessible</p> <p> </p>"},{"location":"40_initialization/#github","title":"GitHub","text":"<p>You'll need to create a GitHub Organization with your DANDI fork. See here for documentation to create a GitHub organization</p>"},{"location":"40_initialization/#initialize-your-oauth-app","title":"Initialize your OAuth App","text":"<p>Once you create the Organization, navigate to the <code>Settings</code> tab:</p> <p> </p> <p>Under <code>Settings</code>, you'll want to initialize an <code>OAuth App</code> -- navigate to <code>Developer settings &gt; OAuth Apps</code></p> <p> </p> <p>Click on <code>New Org OAuth App</code> next</p> <p> </p> <p>You'll be prompted with the following form -- see the example values populated in this screenshot -- more to come in other sections for where these values might be populated:</p> <p> </p>"},{"location":"40_initialization/#obtaining-your-oauth-app-credentials","title":"Obtaining your OAuth App Credentials","text":"<p>After creating your OAuth App, you'll lastly want to obtain a client secret key and your client ID -- make sure to note these values for further steps when creating our API</p> <p> </p>"},{"location":"40_initialization/#connecting-github-with-heroku","title":"Connecting GitHub with Heroku","text":"<p>This step is contingent that you have forked dandi-archive</p> <p>Once you have the repository forked, you'll want to navigate to <code>Settings</code> in the repository page:</p> <p> </p> <p>Next, navigate to <code>Security &gt; Secrets and variables &gt; Actions</code></p> <p> </p> <p>And then lastly create two new secrets, <code>HEROKU_EMAIL</code> should be the email address/account you used to generate your <code>HEROKU_API_KEY</code></p> <p> </p>"},{"location":"40_initialization/#terraform-cloud","title":"Terraform Cloud","text":"<p>Terraform is configuration tool for managing \"infrastructure-as-code\" -- meaning that we can programatically manage infrastructure in a traceable, version-controlled form.</p> <p>The Terraform ecosystem provides a UI tool called Terraform Cloud</p> <p>Start by visiting <code>https://app.terraform.io/</code> and making an account.</p>"},{"location":"40_initialization/#creating-a-terraform-project-and-workspace","title":"Creating a Terraform Project and Workspace","text":"<p>Once you have successfully made an account, you'll want to create a <code>Workspace</code> and a <code>Project</code> in that <code>Workspace</code></p> <p> </p>"},{"location":"40_initialization/#populate-environment-variables","title":"Populate Environment Variables","text":"<p>After successful creation, you'll want to populate the following variables in the <code>Variables</code> section of the <code>Workspace</code>:</p> <p>You'll need to populate <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, <code>HEROKU_API_KEY</code> and <code>HEROKU_EMAIL</code></p> <p> </p>"},{"location":"40_initialization/#link-github-repository","title":"Link GitHub Repository","text":"<p>Next, link your appropriate GitHub repository, in this case, most likely your fork of dandi-infrastructure.</p> <p> </p> <p>Link your repository -- you may need to declare the appropriate subdirectory </p> <p> </p> <p>You can now invoke Terraform <code>plan</code> and <code>apply</code> from the Terraform Cloud UI or via your GitHub repository Actions.</p>"},{"location":"40_initialization/#netlify","title":"Netlify","text":"<p>The frontend for <code>dandi-archive</code> is served via Netlify</p>"},{"location":"40_initialization/#create-account-and-initialize-project","title":"Create Account and Initialize Project","text":"<p>First, create an account. After creating an account, you'll want to navigate to <code>Sites</code>, where you can <code>Add a new site</code>, and then <code>Import an existing project</code></p> <p> </p> <p>You'll want to next connect <code>linc-archive</code> via <code>Deploy with GitHub</code>. Enable Netlify to be authorized as a GitHub app. Once you have enabled authorization, you'll need to specifically link your appropriate repositories:</p> <p> </p> <p>Now once you see the appropriate repository, you'll want to navigate to configure where Netlify can find and build your site</p> <p> </p> <p>For usage of Netlify, one could refer to declaring a <code>netlify.toml</code> configuration file like the one referenced in DANDI Archive</p> <p>These values can also be replicated in the settings. </p> <p> </p> <p>Your frontend should be able to deploy to a auto-generated URL via Netlify now! Steps for domain management and configuration are described further in the Frontend Deployment section of these docs.</p>"},{"location":"40_initialization/#sentry","title":"Sentry","text":"<p>Sentry is a monitoring tool used for the DANDI Archive API. It is integral in order to notify engineers if a system is down, experiencing poor performance, or may have unwanted users</p> <p>Begin by creating a Sentry account -- once successful, you'll start by creating a new Project:</p> <p> </p>"},{"location":"40_initialization/#select-django-as-an-app-type","title":"Select Django as an App Type","text":"<p>DANDI Archive API is built as a Django app -- so proceed to select <code>Django</code> on the following screen:</p> <p> </p>"},{"location":"40_initialization/#capture-sentry-dsn-value","title":"Capture Sentry DSN value","text":"<p>You'll be provided with a screen displaying how to initialize and install Sentry into your Django app. For now, just capture the DSN value. This value will be used later as an environment variables while deploying your API via Terraform</p> <p> </p>"},{"location":"40_initialization/#pypi","title":"PyPI","text":"<p>A common way that users interact with the DANDI ecosystem is via its CLI and Python client tool: dandi-cli</p>"},{"location":"40_initialization/#create-a-pypi-account","title":"Create a PyPI Account","text":"<p>Versions of this tool are hosted, versioned and managed by PyPI -- therefore, you will need to create an account.</p>"},{"location":"40_initialization/#retrieve-an-api-token","title":"Retrieve an API Token","text":"<p>After creating an account, you'll want to create your own API key -- first go to <code>Account settings</code></p> <p> </p> <p>Scroll down to <code>API tokens</code> and create a value -- keep track of this value for now. We will use it later</p> <p> </p> <p>In terms of creating and publishing your first project, go to the documentation for setting up a CLI and Python Client</p>"},{"location":"40_initialization/#datalad-tbd","title":"datalad (TBD)","text":""},{"location":"40_initialization/#git-annex-tbd","title":"git-annex (TBD)","text":""},{"location":"41_api_deployment/","title":"API Deployment","text":""},{"location":"41_api_deployment/#initial-steps","title":"Initial Steps","text":"<p>This section assumes that you have completed all steps in Linc Development &gt; Initialization</p> <p>For the purpose of LINC coupling to DANDI Archive, you'll first want to fork the dandi-infrastructure repository.</p>"},{"location":"41_api_deployment/#understanding-dandi-infrastructure","title":"Understanding \"dandi-infrastructure\"","text":"<p><code>dandi-infrastructure</code> is a repository containing a handful of Terraform configuration files. The baseline for all these files; however, is an external library, known as <code>girder</code>. <code>girder</code> is a series of Terraform modules maintained by Kitware</p> <p><code>girder</code> is sub-divided further into specific Terraform providers that interact with the module. In the case of LINC Brain, </p>"},{"location":"41_api_deployment/#first-steps-for-deployment-of-staging-api","title":"First Steps for Deployment of Staging API","text":"<p>\u2022 In your relevant Terraform GitHub repository, enter a new <code>project_slug</code> within <code>api.tf</code> in the <code>api</code> section -- this will be the name of the application that will be initialized in Heroku</p> <p>\u2022 (For staging), go into your Dandi Archive-related, and alter the application name in the \"Create Heroku Build\" step <code>backend-staging-deploy.yml</code> GitHub Action Workflow</p> <p>\u2022 Next, run your Terraform Plan via Terraform Cloud -- a shortcoming of the current setup is that the Deployment will initially fail - You'll next need to \"Deploy\" the Dandi-related Archive app via the <code>backend-staging-deploy.yml</code> workflow</p> <p>\u2022 Next, run your Terraform Plan one more time - This will notice the <code>Procfile</code> in your Dandi-related Archive, in which the appropriate Heroku Dyno workers will then be properly provisioned</p> <p>\u2022 The API working correctly relies on a handful of infrastructure being set up, as well as specific authentication variables being set....</p>"},{"location":"46_local_dev/","title":"Local Development of These Docs","text":"<p>If you'd like to develop these docs, the follow commands should get you started locally -- with hot-reloading too :)</p> <p>You'll need to create and configure a Python environment according to configuration provided in <code>requirements.txt</code> file in the root of this directoy, e.g. via</p> <pre><code>python3 -m venv venv &amp;&amp; source venv/bin/activate &amp;&amp; python3 -m pip install -r requirements.txt\n</code></pre> <p>And your current session would already be using that virtual Python environment, which you could deactivate by executing <code>deactivate</code> command. If in the future you would need to activate it, just <code>source venv/bin/activate</code> again.</p> <p>After that you can either - do one time manual build using <code>mkdocs build</code> and find built website under <code>site/</code> folder. - run <code>mkdocs serve</code> which would not only build website and start a local webserver for you to visit rendered version at e.g., http://127.0.0.1:8000/</p>"},{"location":"about/policies/","title":"General Policies v1.0.1","text":""},{"location":"about/policies/#content","title":"Content","text":"<ul> <li>Scope: Neurophysiology research. Raw and derived experimental data. Content   must not violate privacy or copyright, or breach confidentiality or non-disclosure   agreements for data collected from human subjects.</li> <li>Status of research data: Empirical (not simulated) data and associated metadata from any stage of the   research study's life cycle is accepted.  Simulated data is handled on a case-by-case basis, contact the DANDI team </li> <li>Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are   allowed to deposit content for which they possess the appropriate rights   and which falls within the scope of the archive.</li> <li>Ownership: By uploading content, no change of ownership is implied and no   property rights are transferred to the DANDI team. All uploaded content remains   the property of the parties prior to submission and must be accompanied by a license allowing   DANDI project data access, archival, and re-distribution (see License below).</li> <li>Data file formats: DANDI only accepts data using standardized formats such   as Neurodata Without Borders, Brain Imaging Data Structure,   Neuroimaging Data Model, and other BRAIN Initiative   standards. We are working with the community to improve these standards and to   make DANDI archive FAIR.</li> <li>Volume and size limitations: There is a limit of 5TB per file. We currently   accept any size of standardized datasets, as long as you can upload them over   an HTTPS connection. However, we ask you contact us if you plan to upload more than 10TB of data.</li> <li>Data quality: All data are provided \u201cas-is\u201d, and the user shall hold   DANDI and data providers supplying data to the DANDI Archive free and harmless in   connection with the use of such data.</li> <li>Metadata types and sources: All metadata is stored internally in JSON format   according to a defined JSON schema. Metadata records violating the schema are not allowed.</li> <li>Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate.</li> <li>Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license.</li> </ul>"},{"location":"about/policies/#access-and-reuse","title":"Access and Reuse","text":"<ul> <li>Access to data objects: Files deposited to the archive are accessible to the public    openly or accessible to collaborators for embargoed datasets. Access to metadata and data    files is provided over standard protocols such as HTTPS.</li> <li>Use and reuse of data objects: Use and reuse is subject to the terms of the license   under which the data objects were deposited.</li> <li> <p>Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested.</p> </li> <li> <p>Embargo status: Users may deposit content under an embargo status and   provide an anticipated end date for the embargo. The repository will restrict   access to the data until the end of the embargo period, at which time the   content will automatically become publicly available. The end of the embargo   period is the earliest of the date provided by submitter, the first publication   using the data, or the end of funding support for the collection and/or dissemination   of the dataset.</p> </li> <li>Restricted access: Depositors of embargoed datasets have the ability to   share access with other collaborators. These files will not be made publicly   available till the end of the embargo period.</li> </ul>"},{"location":"about/policies/#removal","title":"Removal","text":"<ul> <li> <p>Revocation: Content not considered to fall under the scope of the repository   can be removed and associated DOIs issued by DANDI revoked. Inform the DANDI team   promptly, ideally no later than 24 hours from upload, about any suspected policy   violation. Alternatively, content found to already have an external DOI will   have the DANDI DOI invalidated and the record updated to indicate the original   external DOI. User access may be revoked on violation of Terms of Use.</p> </li> <li> <p>Withdrawal: If the uploaded research object must later be withdrawn, the   reason for the withdrawal will be indicated on a tombstone page, which will   henceforth be served in its place. Withdrawal is considered an exceptional   action, which normally should be requested and fully justified by the original   uploader. In any other circumstance reasonable attempts will be made to contact   the original uploader to obtain consent. The DOI and the URL of the original   object are retained.</p> </li> <li> <p>User data on Dandihub: At present, user data on Dandihub is being removed   periodically and Dandihub storage space should not be considered persistent.</p> </li> </ul>"},{"location":"about/policies/#longevity","title":"Longevity","text":"<ul> <li>Versions: Datasets are versioned when published. Prior to publishing the   state of a dataset may continue to evolve and the data or metadata are neither   versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is   never modified.</li> <li>Replicas: All data files are stored on an AWS public bucket, with replicas   housed at Dartmouth College.  Data files are kept in multiple replicas at the   moment, but this may change over time, and no recovery mechanisms for unversioned data   should be assumed to be in place.</li> <li>Retention period: Versioned items will be retained for the lifetime of the repository.   This is currently the lifetime of the NIH award, which currently expires in   April 2029.</li> <li>Functional preservation: DANDI makes no promises of usability and   understandability of deposited objects.</li> <li>File preservation: Data files and metadata are backed up nightly and   replicated into multiple copies in different storage services.</li> <li>Fixity and authenticity: All data files are stored along with multiple   checksums of the file content. Files are regularly checked against their   checksums to assure that file content remains constant.</li> <li>Succession plans: In case of a repository shutdown, our best efforts will   be made to integrate all content into suitable alternative institutional and/or   other repositories overlapping in the scope of the DANDI archive.</li> </ul> <p>This policy document is derived from the Zenodo General Policies v1.0.</p>"},{"location":"about/terms/","title":"Terms of Use v1.0.1","text":"<p>The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work.</p> <p>Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms:</p> <ol> <li> <p>DANDI is an open dissemination research data repository for the preservation    and making available of research, educational and informational content. Access    to DANDI\u2019s content is open to all.</p> </li> <li> <p>Content may be uploaded free of charge by the US BRAIN Initiative and other    projects required to submit data to a public archive and those without ready    access to an organized data center.</p> </li> <li> <p>The uploader is exclusively responsible for the content that they upload to    DANDI and shall indemnify and hold the DANDI team free and harmless in    connection with their use of the service. The uploader shall ensure that their    content is suitable for open dissemination, and that it complies with these    terms and applicable laws, including, but not limited to, privacy, data    protection and intellectual property rights [1]. In addition, where data that    was originally sensitive personal data is being uploaded for open dissemination    through DANDI, the uploader shall ensure that such data is either anonymized    to an appropriate degree or fully consent cleared [2].</p> </li> <li> <p>Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of    content (\"Users\") shall respect applicable license conditions. Download and    use of content from DANDI does not transfer any intellectual property rights    in the content to the User.</p> </li> <li> <p>Users are exclusively responsible for their use of content, and shall indemnify    and hold the DANDI team free and harmless in connection with their download    and/or use. Hosting and making content available through DANDI does not    represent any approval or endorsement of such content by the DANDI team.</p> </li> <li> <p>The DANDI team reserves the right, without notice, at its sole discretion and    without liability, (i) to alter, delete or block access to content that it    deems to be inappropriate or insufficiently protected, and (ii) to restrict    or remove User access where it considers that use of DANDI interferes with    its operations or violates these Terms of Use or applicable laws.</p> </li> <li> <p>Unless specified otherwise, DANDI metadata may be freely reused under the    CC0 waiver.</p> </li> <li> <p>These Terms of Use are subject to change by the DANDI team at any time and    without notice, other than through posting the updated Terms of Use on the    DANDI website.</p> </li> <li> <p>Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted   sensitive personal data are advised to use bespoke platforms rather than open   dissemination services like DANDI for sharing their data.</p> </li> </ol> <p>[1] [2] See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information.</p> <p>If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us.</p> <p>This Terms of Service document is derived from the Zenodo terms of service v1.2.</p>"}]}
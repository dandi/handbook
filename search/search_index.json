{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the DANDI Archive Documentation","text":"<p>The Web interface to the DANDI archive is located at https://dandiarchive.org. This documentation explains how to interact with the archive.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<p>If you want to know more about the DANDI project, its goals, and the problems it tries to solve, check out the Introduction.</p> <p>To start using the archive, head over to Using DANDI in the User Guide section.</p> <p>If are a developer and want to know how the project is organized, check out the Project Structure page in the Developer Guide section.</p>"},{"location":"#where-to-get-help","title":"Where to Get Help","text":"<p>You can communicate with the DANDI team in a variety of ways, depending on your needs:</p> <ul> <li>You can ask questions, report bugs, or  request features at our helpdesk.</li> <li>For interacting with the global neuroscience community, post on https://neurostars.org and use the tag dandi.</li> <li>You can use the DANDI Slack workspace, which we will invite you to after approving your registration on    DANDI using GitHub (this registration is required to upload data or to use the DANDI    JupyterHub). See here for details on how to    register.</li> <li>Email us: info@dandiarchive.org</li> </ul>"},{"location":"#contributing-and-feedback","title":"Contributing and Feedback","text":"<p>We are looking for people to give us feedback on this documentation. If anything is unclear, open an issue on our repository. You can also get in touch on  our Slack channel, which is available to those who have registered an account on the archive.</p> <p>If you want to get started right away and contribute directly to this documentation, see the About This Documentation section.</p>"},{"location":"#license","title":"License","text":"<p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"01_introduction/","title":"Introduction","text":""},{"location":"01_introduction/#what-is-dandi","title":"What is DANDI?","text":"<p>DANDI is:</p> <ul> <li>An open data archive to submit neurophysiology data for electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments.</li> <li>A persistent, versioned, and growing collection of standardized datasets.</li> <li>A place to house data to collaborate across research sites.</li> <li>Supported by the BRAIN Initiative and the AWS Public dataset programs.</li> </ul> <p>DANDI provides significant benefits:</p> <ul> <li>A FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized neurophysiology and associated data.</li> <li>Rich metadata to support search across data.</li> <li>Consistent and transparent data standards to simplify data reuse and software development. We use the Neurodata Without Borders,  Brain Imaging Data Structure, Neuroimaging Data Model (NIDM), and other BRAIN Initiative standards to organize and search the data. See Data Standards for more information.</li> <li>The data can be accessed programmatically allowing for software to work directly with data in the cloud.</li> <li>The infrastructure is built on a software stack of open source products, thus enriching the ecosystem.</li> </ul>"},{"location":"01_introduction/#properties-of-dandi","title":"Properties of DANDI","text":"<p>Data identifiers: The archive provides persistent identifiers for versioned datasets and assets, thus improving reproducibility of neurophysiology research.</p> <p>Data storage: Cloud-based platform on AWS. Data are available from a public S3 bucket. Data from embargoed datasets are available from a private bucket to owners only.</p> <p>Type of data The archive accepts cellular neurophysiology data including electrophysiology, optophysiology, and behavioral time-series, and images from immunostaining experiments and other associated data (e.g. participant information, MRI or other modalities).</p> <p>Accepted Standards and Data File Formats: NWB (HDF5), BIDS (NIfTI, JSON, PNG, TIF, OME.TIF, OME.BTF, OME.ZARR)  (see Data Standards for more details)</p>"},{"location":"01_introduction/#neurophysiology-informatics-challenges-and-dandi-solutions","title":"Neurophysiology Informatics Challenges and DANDI Solutions","text":"Challenges Solutions Most raw data stays in laboratories. DANDI provides a public archive for dissemination of raw and derived data. Non-standardized datasets lead to significant resource needs to understand and adapt code to these datasets. DANDI standardizes all data using NWB and BIDS standards. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. The DANDI ecosystem provides tools for converting data from different instruments into NWB and BIDS. There are many domain general places to house data (e.g. Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant scientific metadata. DANDI is focused on neurophysiology data and related metadata. Datasets are growing larger, requiring compute services to be closer to data. DANDI provides Dandihub, a JupyterHub instance close to the data. Neurotechnology is evolving and requires changes to metadata and data storage. DANDI works with community members to improve data standards and formats. Consolidating and creating robust algorithms (e.g. spike sorting) requires varied data sources. DANDI provides access to many different datasets."},{"location":"100_contribute_docs/","title":"Contributing to this documentation","text":"<p>This documentation is a work in progress and we welcome all input: if something is missing or unclear, let us know by opening an issue on our helpdesk.</p>"},{"location":"100_contribute_docs/#serving-the-docs-locally","title":"Serving the Docs Locally","text":"<p>This project uses the MkDocs tool with the Material theme and extra plugins to generate the website.</p> <p>To test locally, you will need to install the Python dependencies. To do that, type the following commands:</p> <pre><code>git clone https://github.com/dandi/dandi-docs.git\ncd dandi-docs\npip install -r requirements.txt\n</code></pre> <p>If you are working on your fork, simply replace <code>https://github.com/dandi/dandi-docs.git</code> with <code>git clone git@github.com/&lt;username&gt;/dandi-docs.git</code> , where <code>&lt;username&gt;</code> is your GitHub username.</p> <p>Once done, you need to run MkDocs. Simply type:</p> <pre><code>mkdocs serve\n</code></pre> <p>Finally, open <code>http://127.0.0.1:8000/</code> in your browser, and you should see the default home page of the documentation being displayed.</p>"},{"location":"10_using_dandi/","title":"Using DANDI","text":"<p>DANDI allows you to work with stored neurophysiology data in multiple ways. You can search, view, and download files, all without registering for a DANDI account. As a registered user, you can also create these collections of data  along with metadata and publish them to the DANDI platform. </p>"},{"location":"10_using_dandi/#dandisets","title":"Dandisets","text":"<p>DANDI stores cellular neurophysiology data in Dandisets.</p> <p>A Dandiset is a collection of assets (files and their metadata) and metadata about the collection.</p> <ul> <li>A Dandiset is organized in a structured manner to help users and software tools interact with it.</li> <li>Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g. https://identifiers.org/DANDI:000004). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset.</li> </ul>"},{"location":"10_using_dandi/#quick-start","title":"Quick Start","text":"<p>If you are new to DANDI, all you need is an Internet connection to use the DANDI Web application to view  and download files from a  public  Dandiset.  Registration is not required.</p> <p>To view a specific public Dandiset and download one of its files:</p> <ol> <li> <p>At the top of the DANDI Web application, click PUBLIC DANDISETS to see all Dandisets currently available in the     archive. You can sort them by name, identifier, or date of modification.</p> </li> <li> <p>Search for a specific Dandiset by contributor name, modality, or species.</p> </li> <li> <p>Click a Dandiset to open its landing page and view important information such as contact information,     description, license, access information and keywords, and simple statistics.</p> </li> <li> <p>From the right side of the Dandiset landing page, click FILES to see a list of all folders and files for that     Dandiset. Click the download icon  to download a     specific file.  Note: To download an entire Dandiset, you will need to follow the instructions in the     Download section to install and use the DANDI Python client tool.</p> </li> </ol>"},{"location":"10_using_dandi/#next-steps","title":"Next steps","text":"<p>Although anyone on the Internet  can view and download public Dandisets, registered users can also create Dandisets, upload data, and publish  the Dandiset to generate a DOI for it. See the sections that follow for more detailed information about the DANDI project, as well as instructions on how  to work with public Dandisets or to create and publish you own as a registered user. </p>"},{"location":"10_using_dandi/#dandiset-actions","title":"Dandiset Actions","text":"<p>The DANDI project contains the DANDI Web application, the DANDI Python client tool, and the DANDI JupyterHub  instance. These tools can be used to perform actions on Dandisets. </p> <p></p> <p>You can learn more about the Dandiset actions in separate sections:</p> <ul> <li>View</li> <li>Download</li> <li>Upload</li> <li>Publish</li> </ul>"},{"location":"10_using_dandi/#tools-to-interact-with-dandi","title":"Tools to interact with DANDI","text":""},{"location":"10_using_dandi/#dandi-web-application","title":"DANDI Web application","text":"<p>The DANDI Web application allows you to:</p> <ul> <li>Search across all public Dandisets</li> <li>Download data from public Dandisets</li> <li>Create a new Dandiset and provide metadata</li> <li>Publish your Dandiset</li> </ul>"},{"location":"10_using_dandi/#dandi-python-client","title":"DANDI Python client","text":"<p>The DANDI Python client allows you to:</p> <ul> <li>Download Dandisets and individual subject folders or files</li> <li>Organize your data locally before upload</li> <li>Upload Dandisets</li> </ul> <p>Before you can use the DANDI Python client, you have to install the package with <code>pip install dandi</code> in a Python 3.8+ environment.</p> <p>You should check the Dandi Debugging section in case of any problems.</p>"},{"location":"10_using_dandi/#dandihub-analysis-platform","title":"Dandihub analysis platform","text":"<p>Dandihub provides a JupyterHub instance in the cloud to interact with the data stored in DANDI.</p> <p>To use the hub, you will need to register for an account using the DANDI Web application.  Note that <code>Dandihub</code> is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"10_using_dandi/#technical-limitations","title":"Technical limitations","text":"<ul> <li>File name/path: There is a limit of 512 characters for the full path length within a dandiset.</li> <li>Volume and size: There is a limit of 5TB per file. We currently   accept any size of standardized datasets, as long as you can upload them over   an HTTPS connection. However, we ask you contact us if you plan to upload more than 10TB of data.</li> </ul>"},{"location":"10_using_dandi/#citing-dandi","title":"Citing DANDI","text":"<p>You can add the following statement to the methods section of your manuscript.</p> <p>Data and associated metadata were uploaded to the DANDI archive [RRID:SCR_017571] using    the Python command line tool (https://doi.org/10.5281/zenodo.3692138). The data were first    converted into the NWB format (https://doi.org/10.1101/2021.03.13.435173) and  organized    into a BIDS-like (https://doi.org/10.1038/sdata.2016.44) structure.</p> <p>You can refer to DANDI using any of the following options:</p> <ul> <li> <p>Using an RRID RRID:SCR_017571. </p> </li> <li> <p>Using the DANDI CLI reference: https://doi.org/10.5281/zenodo.3692138</p> </li> </ul>"},{"location":"11_view/","title":"Viewing Dandisets","text":""},{"location":"11_view/#browse-dandisets","title":"Browse Dandisets","text":"<p>When you go to the DANDI Web application, you can click on <code>PUBLIC DANDISET</code> to access all Dandisets currently available  in the archive, and you can sort them by name, identifier, size, or date of modification.</p> <p></p>"},{"location":"11_view/#search-dandisets","title":"Search Dandisets","text":"<p>In addition, you can search across the Dandisets for any text part of the Dandiset metadata record.  The text may be about contributor names, modalities, or species.  For example,  <code>\"house mouse\"</code> will return a subset of all Dandisets, while <code>\"mouse house\"</code> will likely not return any. When unquoted each word is used as an <code>OR</code>.</p> <p></p> <p>When you click on one of the Dandisets, you can see that the searching phrase can appear in the description, keywords, or in the assets summary.</p> <p></p>"},{"location":"11_view/#dandisets-metadata","title":"Dandisets Metadata","text":"<p>The landing page of each Dandiset contains important information including  metadata provided by the owners such as contact information, description, license, access information and keywords,   simple statistics for a Dandiset such as size of the Dandiset and number of files, or  a summary of the Dandiset including information about species, techniques, and standards.</p> <p></p> <p>If you scroll down, you will also find: - Assets Summary - Funding Information - Related Resources</p> <p>While most of the metadata is summarized on the landing page, some additional information can be  found by clicking <code>Metadata</code> on the right-side panel. For Dandiset owners, this button also allows  adding relevant metadata to populate the landing page.</p> <p></p>"},{"location":"11_view/#file-view","title":"File View","text":"<p>The right side panel allows you also to access a file browser to navigate the list of folders and files in a Dandiset.</p> <p></p> <p>Any file in the Dandiset has a download icon You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. In addition, there is an info icon that leads to full asset metadata. Some files also have a link to external  services that can open the file. Note: that these services often have size limits and hence are activated only for appropriately sized files.</p>"},{"location":"11_view/#my-dandisets","title":"My Dandisets","text":"<p>If you log in as a registered user, you will also see <code>My Dandisets</code> tab:</p> <p></p> <p>By clicking the tab, you can access all the Dandisets you own. For these Dandisets, you can edit and update  metadata through the Dandiset actions section, and add or remove other owners or data.</p>"},{"location":"12_download/","title":"Downloading Data and Dandisets","text":"<p>You can download the content of a Dandiset using the DANDI Web application (such a specific file) or entire Dandisets using the DANDI Python CLI.</p>"},{"location":"12_download/#using-the-dandi-web-application","title":"Using the DANDI Web Application","text":"<p>Once you have the Dandiset you are interested in (see more in the Dandiset View section), you can download the content of the Dandiset. On the landing page of each Dandiset, you can find <code>Download</code> button on the right-hand panel. After clicking the button, you will see the specific command you can use with DANDI Python CLI (as well as the information on how to download the CLI).</p> <p></p>"},{"location":"12_download/#download-specific-files","title":"Download specific files","text":"<p>The right-side panel of the Dandiset landing page allows you also to access the list of folders and files.</p> <p></p> <p>Each file in the Dandiset has a download icon next to it, clicking the icon will start the download process.</p>"},{"location":"12_download/#using-the-python-cli-client","title":"Using the Python CLI Client","text":"<p>The DANDI Python client gives you more options, such as downloading entire Dandisets.</p> <p>Before You Begin: You need to have Python 3.8+ and install the DANDI Python Client using <code>pip install dandi</code>. If you have an issue using the Python CLI, see the Dandi Debugging section.</p>"},{"location":"12_download/#download-a-dandiset","title":"Download a Dandiset","text":"<p>To download an entire Dandiset, you can use the same command as suggested by DANDI web application, e.g.:</p> <pre><code>dandi download DANDI:000023\n</code></pre>"},{"location":"12_download/#download-data-for-a-specific-subject-from-a-dandiset","title":"Download data for a specific subject from a Dandiset","text":"<p>You can download data for specific subjects. Names of the subjects can be found on DANDI web application or by running a command with the DANDI CLI: <code>dandi ls -r DANDI:000023</code>. Once you have the subject ID, you can download the data, e.g.:</p> <pre><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/draft/assets/?path=sub-811677083\n</code></pre> <p>You could replace <code>draft</code> with a specific non-draft version you are interested in (e.g. <code>0.210914.1900</code> in the case of this Dandiset), if you are not interested in the latest, possibly different state of the Dandiset.</p> <p>You can also use the link from DANDI web application, e.g.:</p> <pre><code>dandi download https://dandiarchive.org/dandiset/000023/0.210914.1900/files?location=sub-541516760%2F\n</code></pre>"},{"location":"12_download/#download-a-specific-file-from-a-dandiset","title":"Download a specific file from a Dandiset","text":"<p>You can download a specific file from a Dandiset when the link for the specific file can be found on the DANDI web application, e.g.:</p> <pre><code>dandi download https://api.dandiarchive.org/api/dandisets/000023/versions/0.210914.1900/assets/1a93dc97-327d-4f9c-992d-c2149e7810ae/download/\n</code></pre> <p>Hint: <code>dandi download</code> supports a number of Resource Identifiers to point to a Dandiset, folder, or file.  Providing an incorrect URL (e.g. <code>dandi download wrongurl</code>) will provide a list of supported identifiers.</p>"},{"location":"12_download/#download-the-dandisetyaml-file-and-a-specific-file-within-the-directory-tree-of-the-dandiset","title":"Download the <code>dandiset.yaml</code> file and a specific file within the directory tree of the Dandiset","text":"<p>Now available in version <code>0.63.0</code> is the <code>--preserve-tree</code> option. In the command below, replace the <code>&lt;dandiset-id&gt;</code>, <code>&lt;version&gt;</code>, and asset <code>&lt;path&gt;</code>. The <code>&lt;path&gt;</code> can be found by selecting the <code>View asset metadata</code> icon next to an asset on https://dandiarchive.org and locating the <code>path</code> key.</p> <pre><code>dandi download --preserve-tree dandi://dandi/&lt;dandiset-id&gt;@&lt;version&gt;/&lt;path&gt;\n</code></pre> <p>For example:</p> <pre><code>dandi download --preserve-tree dandi://dandi/000026@draft/sub-I58/ses-Hip-CT/micr/sub-I58_sample-01_chunk-01_hipCT.json\n</code></pre>"},{"location":"12_download/#using-datalad","title":"Using DataLad","text":"<p>All dandisets are regularly mirrored to DataLad datasets which are made available at the GitHub organization https://github.com/dandisets. Where present, individual Zarr files are included as subdatasets (git submodules) hosted in the GitHub organization https://github.com/dandizarrs/.</p> <p>The Git revision histories of each dataset reflect the Dandiset's draft state as of each execution of the mirroring job. Published Dandiset versions are tagged with Git tags.</p> <p>With DataLad, you can: - clone an entire dataset, - use a specific version of it, - explore history of modifications, - download content of files of interest, - locally discard the content of no-longer-needed files, - use the dataset in a reproducible manner, - include it as a subdataset in your own DataLad dataset, - use https://github.com/datalad/datalad-fuse/ to FUSE-mount individual locally-cloned dandisets so that their files' contents are transparently streamed to your DANDI/DataLad-unaware tools, - etc.</p> <p>Learn more about DataLad from its handbook at https://handbook.datalad.org/.</p> <p>Developers' note: DataLad datasets are created using the dandi/backups2datalad tool which is also available for use by the community to similarly maintain mirrors of independent DANDI deployments as DataLad datasets.</p>"},{"location":"12_download/#using-webdav","title":"Using WebDAV","text":"<p>DANDI provides a WebDAV service at https://webdav.dandiarchive.org/ for accessing the data in the DANDI archive. You can use any WebDAV client or even a web browser to access the data - any dandiset, any version, any file or collection of files. You can use any web download tool to download the data from the DANDI archive, e.g.</p> <pre><code>wget -r -np -nH --cut-dirs=3 https://webdav.dandiarchive.org/dandisets/000027/releases/0.210831.2033/\n</code></pre> <p>for a download of a specific release <code>0.210831.2033</code> of the <code>000027</code> dandiset.</p> <p>Note: The WebDAV service does not directly serve any file contents; it instead relies on redirects to AWS S3 storage where the contents are stored. You might need to configure your WebDAV client to follow redirects; e.g., for the davfs2 WebDAV client, set <code>follow_redirect</code> to <code>1</code> in <code>/etc/davfs2/davfs2.conf</code>.</p> <p>Developers' note: The WebDAV service's code is available at https://github.com/dandi/dandidav/ and can also be used for independent DANDI deployments.</p>"},{"location":"135_validation/","title":"Validation Levels for NWB Files","text":"<p>To be accepted by DANDI, NWB files must conform to criteria that are enforced via three levels of validation:</p>"},{"location":"135_validation/#nwb-file-validation","title":"NWB File Validation","text":"<p>PyNWB validation is used to validate the NWB files,  ensuring that they meet the specifications of core NWB and of any NWB extensions that were used. Generally  speaking, all files produced by PyNWB and MatNWB should pass validation, however there are occasional bugs. More  often, NWB files that fail to meet these criteria have been created outside PyNWB and MatNWB.</p>"},{"location":"135_validation/#critical-nwb-checks","title":"Critical NWB Checks","text":"<p>The NWB Inspector scans NWB files using heuristics to find mistakes  or areas for improvements in NWB files. There are three levels of importance for checks: CRITICAL, BEST PRACTICE  VIOLATIONS, and BEST PRACTICE SUGGESTIONS. CRITICAL warnings indicate some internal inconsistency in the data of the  NWB files. The NWB Inspector will print out all warnings, but only CRITICAL warnings will prevent a file from being  uploaded to DANDI. Errors in NWB Inspector will be block upload as well, but reflect a problem with the NWB  Inspector software as opposed to the NWB file. </p>"},{"location":"135_validation/#missing-dandi-metadata","title":"Missing DANDI Metadata","text":"<p>DANDI has requirements for metadata beyond what is strictly required for NWB validation. The following metadata must  be present in the NWB file for a successful upload to DANDI: - You must define a <code>Subject</code> object. - The <code>Subject</code> object must have a <code>subject_id</code> attribute. - The <code>Subject</code> object must have a <code>species</code> attribute. This can either be the Latin binomial, e.g. \"Mus musculus\", or    an NCBI taxonomic identifier. - The <code>Subject</code> object must have a <code>sex</code> attribute. It must be \"M\", \"F\", \"O\" (other), or \"U\" (unknown). - The <code>Subject</code> object must have either <code>date_of_birth</code> or <code>age</code> attribute. It must be in ISO 8601 format, e.g. \"P70D\"    for 70 days, or, if it is a range, must be \"[lower]/[upper]\", e.g. \"P10W/P12W\", which means \"between 10 and 12 weeks\"</p> <p>These requirements are specified in the  DANDI configuration file of NWB Inspector.</p> <p>Passing all of these levels of validation can sometimes be tricky. If you have any questions, please ask them via the  DANDI Help Desk and we would be happy to assist you.</p>"},{"location":"136_metadata/","title":"Dandiset Metadata","text":"<p>The Dandiset Landing Page (DLP) displays metadata about the Dandiset and the data within. Some of this metadata is automatically extracted from the files, for example the <code>Size</code> in the top panel. The Assets Summary panel at the bottom of the DLP displays additional information that is automatically extracted from NWB files, such as the species of the subject and the types of recordings. This metadata will automatically be re-computed if you make any changes to the data files.</p>"},{"location":"136_metadata/#metadata-editor","title":"Metadata editor","text":"<p>DANDI also has metadata that you must set manually using the <code>METADATA</code> button on the right panel. Any <code>Owner</code> of a Dandiset has the ability to edit the manual Dandiset metadata through this editor. Several fields here are essential for publication, and the rest provide opportunities to make your Dandiset more FAIR, enabling secondary analysis.</p>"},{"location":"136_metadata/#general","title":"General","text":"<p>The General section is for high-level metadata. The title should be about as descriptive as the title of a journal article. If this Dandiset is associated with a specific journal article or preprint, you may give the Dandiset the same title. You may also use the paper abstract as the Dandiset Description, though you are encouraged to provide more detailed information if appropriate.</p> <p>Note that funding information should be entered in the Dandiset Contributors section, not in the <code>Acknowledgements</code> field of <code>General</code>.</p>"},{"location":"136_metadata/#contributors","title":"Contributors","text":"<p>The Contributors section allows you to provide attribution to personnel and organizations that contributed to the publication of this data, metadata that is essential for Publication of the Dandiset. For each person, you have the ability to enter rich metadata. We highly encourage the use of ORCID identifiers for each person. For each person, you have the ability to annotate multiple roles. The \"Author\" role is often appropriate for contributors. Marking a contributor as \"Author\" will make their name appear on the DLP. You can also add contributors that are non-authors, such as data curators. One of the personnel must be listed as the Contact, and this Person must have contact information.</p> <p>The Contributors tab is also where you enter funding information. Choose \"Organization\" and proceed to fill in information. ror.org is a platform that provides unique identifiers for institutions. It is highly recommended to search http://ror.org and include the unique identifiers for each funding agency. Make sure to inspect the metadata for each institution on the ror.org website, as many organizations have similar names (e.g. many different countries have an \"NIH\").</p>"},{"location":"136_metadata/#subject-matter","title":"Subject Matter","text":"<p>The Subject Matter section allows you to annotate the Dandiset with links to terms in ontologies. Use <code>Generic Type</code> for any type other than <code>Anatomy</code> or <code>Disorder</code>.</p>"},{"location":"136_metadata/#ethics-approvals","title":"Ethics Approvals","text":"<p>The Ethics Approvals section allows you to provide information about the ethics approvals that were obtained for the experiment. It is highly recommended to include this information.</p>"},{"location":"136_metadata/#related-resources","title":"Related Resources","text":"<p>This section allows you to annotate the Dandiset with links to related resources such as publications and code repositories. It is highly recommended to add links to the following resources (if they exist):</p> <ul> <li>The associated publication</li> <li>The public code repository used to convert the data</li> <li>A data analysis library associated with the publication that can take this data as input</li> <li>An example notebook submitted to http://github.com/dandi/example-notebooks that demonstrates how to use the data</li> <li>Associated datasets published on DANDI or on other archives.</li> </ul>"},{"location":"13_upload/","title":"Creating Dandisets and Uploading Data","text":"<p>This page provides instructions for creating a new Dandiset and uploading data to DANDI.</p>"},{"location":"13_upload/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Convert data to NWB. You should start by converting your data to NWB format (2.1+). We suggest beginning the conversion process using only a small amount of data so that common issues may be spotted earlier in the process.   This step can be complex depending on your data. Consider using the following tools:</p> <ol> <li>NWB Graphical User Interface for Data Entry (GUIDE) is a cross-platform desktop application for converting data from common proprietary formats to NWB and uploading it to DANDI.</li> <li>NeuroConv is a Python library that automates  conversion to NWB from a variety of popular formats. See the Conversion Gallery for example conversion scripts.</li> <li>PyNWB and MatNWB are APIs in Python and MATLAB that allow full flexibility in reading and writing data. (PyNWB tutorials, MatNWB tutorials)</li> <li>NWB Overview Docs points to more tools helpful for working with NWB files.</li> </ol> <p>Feel free to reach out to us for help.</p> </li> <li> <p>Choose a server.</p> <ul> <li>Production server: https://dandiarchive.org. This is the main server for DANDI and should be used for sharing neuroscience data.   When you create a Dandiset, a permanent ID is automatically assigned to it.   This Dandiset can be fully public or embargoed according to NIH policy.   All data are uploaded as draft and can be adjusted before publishing on the production server.</li> <li>Development server: https://gui-staging.dandiarchive.org. This server is for testing and learning how to use DANDI.   It is not recommended for sharing data, but is recommended for testing the DANDI CLI and GUI or as a testing platform for developers.   Note that the development server should not be used to stage your data.</li> </ul> <p>The below instructions will alert you to where the commands for interacting with these two different servers differ slightly. </p> </li> <li> <p>Register for DANDI and copy the API key. To create a new Dandiset and upload your data, you need to have a DANDI account.</p> <ul> <li>If you do not already have an account, see Create a DANDI Account page for instructions. </li> <li>Once you are logged in, copy your API key.  Click on your user initials in the top-right corner after logging in.  Production (https://dandiarchive.org) and staging (https://gui-staging.dandiarchive.org) servers have different API keys and different logins.</li> <li>Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below.</li> </ul> </li> </ol>"},{"location":"13_upload/#data-uploadmanagement-workflow","title":"Data upload/management workflow","text":"<p>The NWB GUIDE provides a graphical interface for inspecting and validating NWB files, as well as for uploading data to DANDI. See the NWB GUIDE Dataset Publication Tutorial for more information.</p> <p>The below instructions show how to do the same thing programmatically using the command line interface (CLI). The CLI approach may be more suitable for users who are comfortable with the command line or who need to automate the process, or for advanced use-cases.</p> <ol> <li>Create a new Dandiset. <ul> <li>Click <code>NEW DANDISET</code> in the Web application (top right corner) after logging in.</li> <li>You will be asked to enter basic metadata: a name (title) and description (abstract) for your dataset. </li> <li>After you provide a name and description, the dataset identifier will be created; we will call this <code>&lt;dataset_id&gt;</code>.</li> </ul> </li> <li> <p>Check your files for NWB Best Practices.    Run NWB Inspector programmatically. Install the Python library (<code>pip install -U nwbinspector</code>) and run:</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi\n</code></pre> <p>If the report is too large to efficiently navigate in your console, you can save a report using</p> <pre><code>nwbinspector &lt;source_folder&gt; --config dandi --report-file-path &lt;report_location&gt;.txt\n</code></pre> <p>For more details and other options, run:</p> <pre><code>nwbinspector --help\n</code></pre> <p>Thoroughly read the NWBInspector report and try to address as many issues as possible.  DANDI will prevent validation and upload of any issues labeled as level 'CRITICAL' or above when using the <code>--config dandi</code> option.  See \"Validation Levels for NWB Files\" for more information about validation criteria for   uploading NWB files and which are deemed critical. We recommend regularly running the inspector early in the process to generate the best NWB files possible. Note that some auto-detected violations, such as <code>check_data_orientation</code>, may be safely ignored in the event   that the data is confirmed to be in the correct form. See the NWBInspector CLI documentation for more information.</p> </li> <li> <p>Install the DANDI Client.</p> <pre><code>pip install -U dandi\n</code></pre> </li> <li> <p>Validate NWB files. Perform a validation of the NWB files by running:</p> <pre><code>dandi validate --ignore DANDI.NO_DANDISET_FOUND &lt;source_folder&gt;\n</code></pre> <p>If you are having trouble with validation, make sure the conversions were run with the most recent version of <code>dandi</code>, <code>PyNWB</code> and <code>MatNWB</code>.</p> </li> <li> <p>Upload the data to DANDI. This can either be done through the NWB GUIDE, or programmatically:</p> <pre><code>dandi download https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft\ncd &lt;dataset_id&gt;\ndandi organize &lt;source_folder&gt; -f dry\ndandi organize &lt;source_folder&gt;\ndandi validate .\ndandi upload\n</code></pre> <p>Note that the <code>organize</code> steps should not be used if you are preparing a BIDS dataset with the NWB files.  Uploading to the development server is controlled via <code>-i</code> option, e.g. <code>dandi upload -i dandi-staging</code>.  Note that validation is also done during <code>upload</code>, but ensuring compliance using <code>validate</code> prior to upload helps avoid interruptions of the lengthier upload process due to validation failures.  If you have an issue using the DANDI Client, see the DANDI Debugging section.</p> </li> <li> <p>Add metadata to the Dandiset. Visit your Dandiset landing page:    <code>https://dandiarchive.org/dandiset/&lt;dataset_id&gt;/draft</code> and click on the <code>METADATA</code> link.</p> </li> </ol>"},{"location":"13_upload/#storing-access-credentials","title":"Storing Access Credentials","text":"<p>There are two options for storing your DANDI access credentials.</p> <ol> <li> <p><code>DANDI_API_KEY</code> Environment Variable</p> <ul> <li> <p>By default, the DANDI CLI looks for an API key in the <code>DANDI_API_KEY</code>   environment variable.  To set this on Linux or macOS, run:</p> <pre><code>export DANDI_API_KEY=personal-key-value\n</code></pre> </li> <li> <p>Note that there are no spaces around the \"=\".</p> </li> </ul> </li> <li> <p><code>keyring</code> Library</p> <ul> <li> <p>If the <code>DANDI_API_KEY</code> environment variable is not set, the CLI will look up the API     key using the keyring library, which     supports numerous backends, including the system keyring, an encrypted keyfile,     and a plaintext (unencrypted) keyfile.</p> </li> <li> <p>Specifying the <code>keyring</code> backend</p> <ul> <li> <p>You can set the backend the <code>keyring</code> library uses either by setting   the <code>PYTHON_KEYRING_BACKEND</code> environment variable or by filling in   the <code>keyring</code> library's configuration file.</p> </li> <li> <p>IDs for the available backends can be listed by running <code>keyring   --list</code>.</p> </li> <li> <p>If no backend is specified in this way, the library will use the   available backend with the highest priority.</p> </li> <li> <p>If the DANDI CLI encounters an error while attempting to fetch the   API key from the default keyring backend, it will fall back to using   an encrypted keyfile (the <code>keyrings.alt.file.EncryptedKeyring</code>   backend).  If the keyfile does not already exist, the CLI will ask   you for confirmation; if you answer \"yes,\" the <code>keyring</code>   configuration file (if it does not already exist; see above) will be   configured to use <code>EncryptedKeyring</code> as the default backend.  If you   answer \"no,\" the CLI will exit with an error, and you must store the   API key somewhere accessible to the CLI on your own.</p> <ul> <li> <p>Unless a different location is set via the <code>keyring</code>   configuration file, the encrypted keyfile will be located at the   following path:</p> <ul> <li> <p>On Linux and macOS, if the <code>XDG_DATA_HOME</code> environment   variable is set to a nonempty string, the keyfile will be at   <code>$XDG_DATA_HOME/python_keyring/crypted_pass.cfg</code>; otherwise,   it will be at   <code>~/.local/share/python_keyring/crypted_pass.cfg</code>.</p> </li> <li> <p>On Windows, if the <code>LOCALAPPDATA</code> environment variable is   set, the keyfile will be at <code>%LOCALAPPDATA%\\Python   Keyring\\crypted_pass.cfg</code>; otherwise, if the <code>ProgramData</code>   environment variable is set, the keyfile will be at   <code>%ProgramData%\\Python Keyring\\crypted_pass.cfg</code>; otherwise,   it will be at <code>Python Keyring\\crypted_pass.cfg</code> within the   current directory.</p> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Storing the API key with <code>keyring</code></p> <ol> <li> <p>You can store your API key where the <code>keyring</code> library can find it by using   the <code>keyring</code> program: Run <code>keyring set dandi-api-dandi key</code> and enter the   API key when asked for the password for <code>key</code> in <code>dandi-api-dandi</code>.</p> </li> <li> <p>If the API key isn't stored in either the <code>DANDI_API_KEY</code> environment variable   or in the keyring, the CLI will prompt you to enter the API key, and then it   will store it in the keyring.  This may cause you to be prompted further; you   may be asked to enter a password to encrypt/decrypt the keyring, or you may be   asked by your operating system to confirm whether to give the DANDI CLI access to the   keyring.</p> </li> </ol> </li> </ul> </li> </ol>"},{"location":"14_publish/","title":"Publishing Dandisets","text":"<p>Once you create a Dandiset, DANDI will automatically create a <code>draft</code> version of the Dandiset that can be changed as many times as needed by editing the  metadata or uploading new files.</p> <p>When the draft version is ready, you can publish your Dandiset. This results in an immutable snapshot of your Dandiset with its own unique version number that others can cite. If you need to change the data or metadata, you can do so by continuing to modify the draft version and publishing a new version when you are ready.</p> <p>Follow these steps to publish your Dandiset:</p> <ol> <li> <p>Edit the Dandiset metadata, aiming to fix all Dandiset metadata validation    errors, and include any other useful information. For example, you may want    to edit the following fields:</p> <ul> <li>People and funding contributors</li> <li>Protocol information</li> <li>Keywords</li> <li>Related resources such as publications and code repositories</li> </ul> </li> <li> <p>Fix all asset metadata errors by modifying the asset files to eliminate    the errors and re-uploading them.</p> </li> <li> <p>When all the Dandiset metadata and asset metadata errors are fixed, and the Dandiset is made public if it was initially embargoed, the    <code>Publish</code> button (on the right panel of the Dandiset landing page) will    be enabled and turn green. Click the button to publish your Dandiset.</p> </li> <li> <p>In the lower right section of the Dandiset landing page, you should see    the new, published version of your Dandiset listed. Click on that link    to view this version.</p> </li> </ol> <p>NOTE: Dandisets with Zarr assets currently cannot be published. We are  actively working on enabling this feature.</p>"},{"location":"15_debugging/","title":"Debugging","text":"<p>If something goes wrong while using the Python CLI client, the first place to check for more information so that you can file a quality bug report is the logs.  Every command records a copy of its logs in a logfile, the location of which is reported to the user when the command finishes running.  The location of the logs varies by platform, e.g.:</p> <ul> <li>Linux: <code>~/.cache/dandi-cli/log</code> or <code>$XDG_CACHE_HOME/dandi-cli/log</code></li> <li>macOS: <code>~/Library/Logs/dandi-cli</code></li> </ul> <p>Logs are named with a combination of the time at which the <code>dandi</code> command started running and the process ID of the command.</p> <p>Recent versions of the client include all possible debugging information in the logs, but if you're using an older version, only log messages that were printed to the user when the command ran are recorded.  As a result, in order to get complete debugging information, you may have to rerun the problematic command, this time increasing the logging level by passing <code>-l DEBUG</code> or <code>--log-level DEBUG</code> on the command line.  Note that this option goes between the main <code>dandi</code> command and the name of the subcommand:</p> <pre><code># Right:\ndandi -l DEBUG upload\n\n# Wrong:\ndandi upload -l DEBUG\n</code></pre> <p>In addition, many commands can be put into a developer-specific mode for showing raw progress information instead of fancy progress bars.  For the <code>delete</code>, <code>organize</code>, <code>upload</code>, and <code>validate</code> commands, this can be done by setting the <code>DANDI_DEVEL</code> environment variable and passing <code>--devel-debug</code> to the command:</p> <pre><code>DANDI_DEVEL=1 dandi upload --devel-debug\n</code></pre> <p>For the <code>download</code> command, the equivalent is the <code>-f debug</code>/<code>--format debug</code> option:</p> <pre><code>dandi download -f debug\n</code></pre> <p>More advanced users who are familiar with the Python debugger can instruct the client to automatically open the debugger if any errors occur by supplying the <code>--pdb</code> option to the command.  Like the <code>-l</code>/<code>--log-level</code> option, the <code>--pdb</code> option must be placed between <code>dandi</code> and the name of the subcommand.</p>"},{"location":"16_account/","title":"Create a DANDI Account","text":"<p>A DANDI account enhances your capabilities within the DANDI Archive. Without an account, users can freely search, view, and download available datasets. With an account, users can create and edit Dandisets, and use the DANDI Hub to analyze data.</p> <p>DANDI provides two servers:</p> <ul> <li>Main server: https://dandiarchive.org/ - This is the primary platform for most users.</li> <li>Staging server: https://gui-staging.dandiarchive.org/ - Ideal for training and testing purposes.</li> </ul> <p>Accounts are independently managed on each server, allowing users to register on one or both, depending on their testing and deployment needs.</p> <p>DANDI is freely accessible to the neuroscience research community. Membership is usually granted automatically to GitHub accounts with a <code>.edu</code> or similar academic email. If your registration is denied:</p> <ul> <li>With an academic email not linked to your GitHub, please contact help@dandiarchive.org for assistance using this email address.</li> <li>Without an academic email, account approval is still possible under specific circumstances. Appeal decisions at help@dandiarchive.org.</li> </ul>"},{"location":"16_account/#how-to-register-for-a-dandi-account","title":"How to Register for a DANDI Account","text":"<ol> <li>Create a GitHub Account: If not already a GitHub user, sign up here.</li> <li>Register on DANDI: Navigate to the DANDI homepage and click the <code>LOG IN WITH GITHUB</code> button to register using your GitHub account.</li> <li>Confirmation of Review: Post-registration, you will receive an email confirming that your account is under review. Your request will be reviewed within 24 hours.</li> <li>Note: Reviews may extend beyond 24 hours for new GitHub accounts or non-.edu email addresses, particularly if the registration does not describe immediate plans to contribute data.</li> <li>Accessing DANDI: Upon approval, access DANDI by logging in through the <code>LOG IN WITH GITHUB</code> button.</li> </ol> <p>For support or further inquiries, reach out to help@dandiarchive.org.</p>"},{"location":"20_project_structure/","title":"Project Structure","text":"<p>The DANDI project can be represented schematically:</p> <p></p> <p>The Client side contains the DANDI Python CLI and DANDI Web application.</p> <p>The Server side contains a RESTful API and DANDI JupyterHub.</p> <p>The Dandiset is a file organization to store data together with metadata.</p> <p>The DANDI project is organized around several GitHub repositories:</p> Repository Description DANDI Archive Contains the code for deploying the client-side Web application frontend based on the Vue.js framework as well as a Django-based backend to run the DANDI REST API. DANDI JupyterHub Contains the code for deploying a JupyterHub instance to support interaction with the DANDI archive. DANDI Python client Contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. DANDI Docs Provides the contents of this website. helpdesk Contains our community help platform where you can submit issues. schema Provides the details and some supporting code for the DANDI metadata schema. schema Python library Provides a Python library for updating the schema and for creating and validating DANDI objects. website Provides an overview of the DANDI project and the team members and collaborators."},{"location":"30_data_standards/","title":"Data Standards","text":"<p>DANDI requires uploaded data to adhere to community data standards.  These standards help data curators package all the necessary metadata and provide a uniform structure so that data can be more easily understood and reused by future users.  DANDI also leverages these standards to provide features like data validation and automatic metadata extraction and search. DANDI currently supports two data standards: </p> <ul> <li>For cellular neurophysiology, such as electrophysiology and optical physiology, use Neurodata Without Borders (NWB)</li> <li>For neuroimaging data, such as MRI, use Brain Imaging Data Structure (BIDS)</li> </ul> <p>For microscopy data from immunostaining, we are using the BIDS extension for microscopy.</p> <p>To share data on DANDI, you will first need to convert your data to an appropriate standard. If you would like help determining which standard is most appropriate for your data, do not hesitate to reach out using the dandi helpdesk and we would be happy to assist.</p>"},{"location":"30_data_standards/#neurodata-without-borders-nwb","title":"Neurodata Without Borders (NWB)","text":"<p>NWB is a data standard for neurophysiology, providing neuroscientists with a common standard to share, archive, use, and build analysis tools for neurophysiology data. NWB is designed to store a variety of neurophysiology data, including data from intracellular and extracellular electrophysiology experiments, data from optical physiology experiments, and tracking and stimulus data. The NWB team supports APIs in Python (PyNWB) and MATLAB (MatNWB), with tutorials for writing data broken down by experiment type. See the NWB Tutorials page for more details. Also see the NWB Conversion Tools user guide for converting data for automated conversions from several popular proprietary data formats.  The best way to get help from the NWB community is through the NWB user Slack channel.</p>"},{"location":"30_data_standards/#brain-imaging-data-format-bids","title":"Brain Imaging Data Format (BIDS)","text":"<p>BIDS is a way to organize and describe neuroimaging and behavioral data.  See the Getting Started page for instructions for how to convert your neuroimaging data to BIDS.</p> <p>For microscopy and associated MR data, use the BIDS extension for microscopy.</p>"},{"location":"35_data_licenses/","title":"Data Licenses","text":"<p>To create a Dandiset, you must select a license under which to share the data. Because the DANDI Archive provides a platform for open data sharing, the licenses come from Creative Commons, an international nonprofit organization dedicated to establishing, growing, and maintaining a shared commons in the spirit of open source.</p> <p>These licenses enable the dataset's copyright holder to grant permissions to others to share and use the data for a wide range of purposes. The licenses available to users of the Archive are as follows:</p> <ul> <li> <p>Attribution (CC-BY-4.0). This license grants permission to share the data to others and to adapt the data by remixing, transforming, and building upon it, so long as appropriate credit is given to the copyright holder and any changes to the original are clearly indicated. The dataset may be used for any purpose (even commercial ones). Note that this license retains the original copyright while granting permissive access to the data to all others.</p> </li> <li> <p>Public domain dedication (CC0-1.0). This license dedicates the dataset to the public domain, relinquishing copyright and therefore allowing anyone to use the dataset for any purpose without restriction.</p> </li> </ul> <p>You can learn more about the theory of how the Creative Commons licenses operate at their website. If you have any questions or concerns, send a message to help@dandiarchive.org.</p>"},{"location":"40_development/","title":"Developer Notes","text":"<p>This page contains important information for anyone starting development work on the DANDI project.</p>"},{"location":"40_development/#overview","title":"Overview","text":"<p>The DANDI archive dev environment comprises three major pieces of software: <code>dandi-archive</code>, <code>dandi-cli</code>, and <code>dandi-schema</code>.</p>"},{"location":"40_development/#dandi-archive","title":"<code>dandi-archive</code>","text":"<p><code>dandi-archive</code> is the web frontend application; it connects to <code>dandi-api</code> and provides a user interface to all the DANDI functionality.  <code>dandi-archive</code> is a standard web application built with <code>yarn</code>. See the <code>dandi-archive</code> README for instructions on how to build it locally.</p> <p>The Django application makes use of several services to provide essential function for the DANDI REST API, including Postgres (to hold administrative data about the web application itself), Celery (to run asynchronous compute tasks as needed to implement API semantics), and RabbitMQ (to act as a message broker between Celery and the rest of the application).</p> <p>The easiest way to run the API along with its services is through a Docker Compose setup, as detailed in the Develop with Docker quickstart.</p>"},{"location":"40_development/#dandi-cli","title":"<code>dandi-cli</code>","text":"<p><code>dandi-cli</code> is a Python command line tool used to manage downloading and uploading of data with the archive. You may need to use this tool when developing new features for the frontend and backend, but there are other methods of faking data in the system to work with as well. You can install <code>dandi-cli</code> with a command like <code>pip install dandi</code> (then invoke <code>dandi</code> on the command line to run the tool), or build it locally following the instructions in the <code>dandi-cli</code> README.</p>"},{"location":"40_development/#dandi-schema","title":"<code>dandi-schema</code>","text":"<p><code>dandi-schema</code> is a Python library for  creating, maintaining, and validating the DANDI metadata models for dandisets  and assets. You may need to make use of this tool when improving models, or  migrating metadata. You can install <code>dandi-schema</code> with a command like  <code>pip install dandi-schema</code>. When releases are published through dandi-schema,  corresponding json-schemas are generated in the release folder of the dandi schema repo. See the <code>dandi-schema</code> README for instructions on  viewing the schemas.</p>"},{"location":"40_development/#technologies-used","title":"Technologies Used","text":"<p>This section details some foundational technologies used in <code>dandi-archive</code>. Some basic understanding of these technologies is the bare minimum requirement for contributing meaningfully, but keep in mind that the DANDI team can help you get spun up as well.</p> <p>JavaScript/TypeScript. The DANDI archive code is a standard JavaScript web application, but we try to implement new functionality using TypeScript.</p> <p>Vue/VueX. The application's components are written in Vue, and global application state is managed through VueX.</p> <p>Vuetify. The components make heavy use of the Vuetify component library.</p> <p>Python3. The backend code is written in Python 3.</p> <p>Django/drf/drf-yasg. The API infrastructure is implemented through a Django application. This means that application resources must be mapped to Django models, while Django views mediate API responses. The REST endpoints are implemented via Django Rest Framework (DRF), while DRF-YASG is used to generate Swagger documentation.</p> <p>For general help with <code>dandi-archive</code>, contact @waxlamp.</p>"},{"location":"40_development/#deployment","title":"Deployment","text":"<p>The DANDI project uses automated services to continuously deploy both the <code>dandi-api</code> backend and the <code>dandi-archive</code> frontend.</p> <p>Heroku manages backend deployment automatically from the <code>master</code> branch of the <code>dandi-api</code> repository. For this reason it is important that pull requests pass all CI tests before they are merged. Heroku configuration is in turn managed by Terraform code stored in the <code>dandi-infrastructure</code> repository. If you need access to the Heroku DANDI organization, talk to @satra.</p> <p>Netlify manages the frontend deployment process. Similarly to <code>dandi-api</code>, these deployments are based on the <code>master</code> branch of <code>dandi-archive</code>. The <code>netlify.toml</code> file controls Netlify settings. The @dandibot GitHub account is the \"owner\" of the Netlify account used for this purpose; in order to get access to that account, speak to @satra.</p>"},{"location":"40_development/#monitoring","title":"Monitoring","text":""},{"location":"40_development/#services-status","title":"Service(s) status","text":"<p>The DANDI project uses upptime to monitor the status of DANDI provided and third-party services. The configuration is available in .upptimerc.yml of the https://github.com/dandi/upptime repository, which is automatically updated by the upptime project pipelines. Upptime automatically opens new issues if any service becomes unresponsive, and closes issues whenever service comes back online. https://www.dandiarchive.org/upptime/ is the public dashboard for the status of DANDI services.</p>"},{"location":"40_development/#logging","title":"Logging","text":""},{"location":"40_development/#sentry","title":"Sentry","text":"<p>Sentry is used for error tracking main deployment. To access Sentry, login to https://dandiarchive.sentry.io .</p>"},{"location":"40_development/#heroku-papertrail","title":"Heroku &amp; Papertrail","text":"<p>The <code>dandi-api</code> and <code>dandi-api-staging</code> apps have the Papertrail add-on configured to capture logs. To access Papertrail, log in to the Heroku dashboard, proceed to the corresponding app and click on the \"Papertrail\" add-on.</p> <p>A cronjob on the <code>drogon</code> server backs up Papertrail logs as .csv files hourly at <code>/mnt/backup/dandi/papertrail-logs/{app}</code>. Moreover, <code>heroku logs</code> processes per app dump logs to <code>/mnt/backup/dandi/heroku-logs/{app}</code> directory.</p>"},{"location":"40_development/#continuous-integration-ci-jobs","title":"Continuous Integration (CI) Jobs","text":"<p>The DANDI project uses GitHub Actions for continuous integration. Logs for many of the repositories are archived on <code>drogon</code> server at <code>/mnt/backup/dandi/tinuous-logs/</code>.</p>"},{"location":"40_development/#code-hosting","title":"Code Hosting","text":"<p>All code repositories are hosted on GitHub. The easiest way to contribute is to gain push access to the repositories by talking to @waxlamp; this way, you can create pull requests based on branches within the origin repositories, which in turn allows for Netlify deploy previews and Heroku staging previews to be built.</p> <p>However, this is not strictly required. You can contribute using the standard fork-and-pull-request model, but under this workflow we will lose the benefit of those previews.</p>"},{"location":"40_development/#email-services","title":"Email Services","text":"<p>DANDI Archive maintains several email services to implement the following facilities:</p> <ul> <li>Public email. Users of the Archive can reach the developers for help or to   report problems by sending email to info@dandiarchive.org and   help@dandiarchive.org. These are \"virtual\" email addresses managed by DNS   entries.</li> <li>Transactional email. The Archive sends email to users to manage the signup   process and to inform about special situations such as long running operations,    registration reject/approval, Dandiset embargo and unembargo, changes to   ownership, etc. These are sent via Amazon Simple Email Service (SES),   programmatically from the Archive code.</li> <li>Mass email. The maintainers of the Archive infrequently send   mass email to all users of the Archive to inform about downtime   or other notifications of mass appeal. This function is managed through a   Mailchimp account that requires special procedures to keep it up to date.</li> </ul>"},{"location":"40_development/#dns-entries-for-public-email-addresses","title":"DNS Entries for public email addresses","text":"<p>The email addresses info@dandiarchive.org and help@dandiarchive.org are advertised to users as general email addresses to use to ask for information or help. These are managed via Terraform as AWS Route 53 MX entries. We use ImprovMX to forward emails sent to these addresses to dandi@mit.edu, a mailing list containing the leaders and developers of the project. (Other virtual addresses within the dandiarchive.org domain can be created as needed.)</p> <p>If you need the credentials for logging into ImprovMX, speak to Roni Choudhury (roni.choudhury@kitware.com).</p>"},{"location":"40_development/#mass-emails-with-mailchimp","title":"Mass emails with Mailchimp","text":"<p>The Archive maintainers are able to send email to all users through the mass emailing functions of a dedicated Mailchimp account. In technical parlance, these communications are also known as \"marketing email\", though as a rule the maintainers do not conduct any actual marketing through this channel. Nonetheless, such communications are governed by laws and regulations such as the American CAN-SPAM Act, the California-specific CCPA, and the European Union's GDPR; Mailchimp helps the maintainers comply with these rules and regulations.</p> <p>Our major use case for mass email is to notify the userbase of upcoming downtime (as is needed for, e.g., a major data migration or maintenance windows).</p> <p>If you need to mass email the DANDI Archive userbase, speak to Roni Choudhury (roni.choudhury@kitware.com).</p>"},{"location":"40_development/#updating-the-dandi-userbase-audience-in-mailchimp","title":"Updating the DANDI userbase audience in Mailchimp","text":"<p>Follow these steps before sending a mass email through Mailchimp to ensure that the Mailchimp-maintained DANDI userbase audience is up to date.</p> <ol> <li>Log into the DANDI admin panel and navigate to the dashboard page (at, e.g.,    <code>api.dandiarchive.org/dashboard</code>).</li> <li>Click on the <code>Mailchimp CSV</code> link in the navbar to download the CSV file to    disk.</li> <li>Log into Mailchimp, click on the <code>Audience</code> section in the sidebar, then    click on the <code>Manage Audience</code> dropdown and select <code>Manage contacts</code>.</li> <li>Click on the <code>Manage audience</code> dropdown and select <code>Archive all contacts</code>.    Then follow the confirmation prompt to carry out the archiving operation.</li> <li>Click on the <code>Add contacts</code> dropdown and select <code>Import contacts</code>.</li> <li>Select the <code>Upload a file</code> option, and follow the wizard steps, uploading the    CSV file from step 2 when prompted. Activate the <code>Update any existing    contacts</code> checkbox under the <code>Organize your contacts</code> step. Do not set any    tags during the <code>Tag your contacts</code> step. In the <code>Match column labels to    contact information</code> step, visually verify that the email address, first    name, and last name columns look correctly matched. In the <code>Subscribe contacts    to marketing</code> step, ensure that <code>Subscribed</code> is selected in the dropdown. In    the <code>Review and complete your import</code>, read over the summary and ensure it is    correct before clicking the <code>Complete Import</code> button to finish the process.</li> </ol> <p>It is necessary to \"deactivate\" the entire userbase before reimporting the current slate of users from the freshly computed CSV file because Mailchimp does not have a way to perform a PUT-like operation during import (to borrow a term from RESTful API design), only to add new users and update existing ones.</p> <p>The reason to archive the contacts instead of deleting them has to do with Mailchimp's semantics for those actions. Deleting a user means they cannot be re-added to the audience, while archiving is a reversible action that retains all data and history and merely removes that user from the audience for purposes of receiving emails. Thus, we use an archive-and-reimport procedure to emulate the PUT-like operation we actually need.</p>"},{"location":"40_development/#miscellaneous-tips-and-information","title":"Miscellaneous Tips and Information","text":""},{"location":"40_development/#use-email-address-to-log-into-dev-django-admin-panel","title":"Use email address to log into dev Django admin panel","text":"<p>Once <code>dandi-api</code> is up and running, you can access the Django admin panel at http://localhost:8000/admin. The login page asks for a \"username\" but really it is expecting the email address associated with the username.</p> <p>One easy trick here is to supply the username again as the email address when you are setting up the superuser during initial setup.</p>"},{"location":"40_development/#refresh-github-login-to-log-into-prod-django-admin-panel","title":"Refresh GitHub login to log into prod Django admin panel","text":"<p>To log into the production Django admin panel, you must simply be logged into the DANDI Archive production instance using an admin account.</p> <p>However, at times the Django admin panel login seems to expire while the login to DANDI Archive proper is still live. In this case, simply log out of DANDI, log back in, and then go to the Django admin panel URL (e.g. https://api.dandiarchive.org/admin) and you should be logged back in there.</p>"},{"location":"40_development/#why-do-incoming-emails-to-dandiarchiveorg-look-crazy","title":"Why do incoming emails to dandiarchive.org look crazy?","text":"<p>When a user emails help@dandiarchive.org or info@dandiarchive.org, those messages are forwarded to dandi@mit.edu (see above) so that the dev team sees them. However, these emails arrive with a long, spammy-looking From address with a Heroku DNS domain; this seems to be an artifact of how mit.edu processes emails, and does not occur in general (e.g. messages sent from the API server to users).</p>"},{"location":"50_hub/","title":"Using the DANDI Hub","text":"<p>DANDI Hub is a JupyterHub instance in the cloud to interact with the data stored in DANDI, and is free to use for exploratory analysis of data on DANDI. For instructions on how to navigate JupyterHub see this YouTube tutorial. Note that DANDI Hub is not intended for significant computation, but provides a place to introspect Dandisets and to perform some analysis and visualization of data.</p>"},{"location":"50_hub/#registration","title":"Registration","text":"<p>To use the DANDI Hub, you must first register for an account using the DANDI website. See the Create a DANDI Account page.</p>"},{"location":"50_hub/#choosing-a-server-option","title":"Choosing a server option","text":"<p>When you start up the DANDI Hub, you will be asked to select across a number of server options. For basic exploration, Tiny or Base would most likely be appropriate. The DANDI Hub also currently offers Medium and Large options, which have more available memory and compute power. The \"T4 GPU inference\" server comes with an associated T4 GPU, and is intended to be used for applications that require GPU for inference. We request that users of this server be considerate of their usage of the DANDI Hub as a free community resource. Training large deep neural networks is not appropriate. A \"Base (MATLAB)\" server is also available, which provides a MATLAB cloud installation but you would be required to provide your own license.</p>"},{"location":"50_hub/#custom-server-image","title":"Custom server image","text":"<p>If you need additional software installed in the image, you can add a server image that will be made available for all users in the <code>Server Options</code> menu.  Add a server image by updating the <code>profileList</code> in the JupyterHub config file and submitting a pull request to the dandi-hub repository.  Once the pull request is merged, the DANDI team will redeploy JupyterHub and the image will be available.</p>"},{"location":"50_hub/#example-notebooks","title":"Example notebooks","text":"<p>The best way to share analyses on DANDI data is through the DANDI example notebooks. These notebooks are maintained in the dandi/example-notebooks repository which provides more information about their organization. Dandiset contributors are encouraged to use these notebooks to demonstrate how to read, analyze, and visualize the data, and how to produce figures from associated scientific publications.</p> <p>Notebooks can be added and updated through a pull request to the dandi/example-notebooks repository. Once the pull request is merged, your contributed notebook will be available to all DANDI Hub users.</p>"},{"location":"citing/","title":"Citing a Dandiset","text":"<p>If you use a Dandiset in your research, please acknowledge the Dandiset by citing it, just as you would a publication, including the DOI. The DOI can be found in the Dandiset's landing page on the DANDI Archive website. An example formatted citation can also be found on the Dandiset's landing page at the \"CITE AS\" button. This citation uses the DataCite citation style, which is a widely accepted standard for citing datasets, but you may need to adapt it to the citation style required by the journal you are submitting to.</p> <p>If the Dandiset has an associated publication, it may also be appropriate to cite the publication, but this does not replace the need to cite the Dandiset itself.</p> <p>Citing the Dandiset and other datasets is important because it provides a direct link to the data used in your research. That is crucial, because it:</p> <ul> <li>allows others to better understand and verify your results, and facilitates reproducibility, </li> <li>connects your work to other research using the same dataset,</li> <li>provides credit to the data collectors and maintainers, </li> <li>helps track the impact of DANDI and other data archives.</li> </ul>"},{"location":"citing/#data-availability-statement","title":"Data availability statement","text":"<p>It is common for journals to require a Data Availability Statement in the manuscript, which should include the DANDI Archive RRID and the DOI of the Dandiset used in the research. Here is an example of a well formatted Data Availability Statement:</p> <p>The data that support the findings of this study are openly available on the DANDI Archive (RRID:SCR_017571) at [DOI of Dandiset] (citation of Dandiset).</p> <p>It is important to note that a Data Availability Statement does not replace the need for a full citation in the manuscript's references section. Both elements serve different purposes and are typically required for comprehensive documentation of data sources.</p>"},{"location":"about/policies/","title":"General Policies v1.1.0","text":""},{"location":"about/policies/#content","title":"Content","text":"<ul> <li>Scope: Neurophysiology research. Raw and derived experimental data. Content   must not violate privacy or copyright, or breach confidentiality or non-disclosure   agreements for data collected from human subjects.</li> <li>Status of research data: Empirical (not simulated) data and associated metadata from any stage of the   research study's life cycle is accepted.  Simulated data is handled on a case-by-case basis, contact the DANDI team </li> <li>Eligible users: Anyone working with the data in the scope of the archive may register as a user of DANDI. All users are   allowed to deposit content for which they possess the appropriate rights   and which falls within the scope of the archive.</li> <li>Ownership: By uploading content, no change of ownership is implied and no   property rights are transferred to the DANDI team. All uploaded content remains   the property of the parties prior to submission and must be accompanied by a license allowing   DANDI project data access, archival, and re-distribution (see License below).</li> <li>Data file formats: DANDI only accepts data using standardized formats such   as Neurodata Without Borders, Brain Imaging Data Structure,   Neuroimaging Data Model, and other BRAIN Initiative   standards. We are working with the community to improve these standards and to   make DANDI archive FAIR.</li> <li>Data quality: All data are provided \u201cas-is\u201d, and the user shall hold   DANDI and data providers supplying data to the DANDI Archive free and harmless in   connection with the use of such data.</li> <li>Metadata types and sources: All metadata is stored internally in JSON format   according to a defined JSON schema. Metadata records violating the schema are not allowed.</li> <li>Language: Textual items must be in English. Latin names could be used in exceptional cases where appropriate.</li> <li>Licenses: Users must specify a license for each dataset chosen from the list of the DANDI archive approved licenses. Users allow for the DANDI archive to extract metadata records and make them available under permissive CC0 license.</li> </ul>"},{"location":"about/policies/#access-and-reuse","title":"Access and Reuse","text":"<ul> <li>Access to data objects: Files deposited to the archive are accessible to the public    openly or accessible to collaborators for embargoed datasets. Access to metadata and data    files is provided over standard protocols such as HTTPS.</li> <li>Use and reuse of data objects: Use and reuse is subject to the terms of the license   under which the data objects were deposited.</li> <li> <p>Metadata access and reuse: Metadata records, provided by the users or extracted from the assets, are licensed under CC0. All metadata is made publicly available and can be harvested.</p> </li> <li> <p>Embargo status: Users may deposit content under an embargo status and   provide an anticipated end date for the embargo. The repository will restrict   access to the data until the end of the embargo period, at which time the   content will automatically become publicly available. The end of the embargo   period is the earliest of the date provided by submitter, the first publication   using the data, or the end of funding support for the collection and/or dissemination   of the dataset.</p> </li> <li>Restricted access: Depositors of embargoed datasets have the ability to   share access with other collaborators. These files will not be made publicly   available till the end of the embargo period.</li> </ul>"},{"location":"about/policies/#removal","title":"Removal","text":"<ul> <li> <p>Revocation: Content not considered to fall under the scope of the repository   can be removed and associated DOIs issued by DANDI revoked. Inform the DANDI team   promptly, ideally no later than 24 hours from upload, about any suspected policy   violation. Alternatively, content found to already have an external DOI will   have the DANDI DOI invalidated and the record updated to indicate the original   external DOI. User access may be revoked on violation of Terms of Use.</p> </li> <li> <p>Withdrawal: If the uploaded research object must later be withdrawn, the   reason for the withdrawal will be indicated on a tombstone page, which will   henceforth be served in its place. Withdrawal is considered an exceptional   action, which normally should be requested and fully justified by the original   uploader. In any other circumstance reasonable attempts will be made to contact   the original uploader to obtain consent. The DOI and the URL of the original   object are retained.</p> </li> <li> <p>User data on Dandihub: At present, user data on Dandihub is being removed   periodically and Dandihub storage space should not be considered persistent.</p> </li> </ul>"},{"location":"about/policies/#longevity","title":"Longevity","text":"<ul> <li>Versions: Datasets are versioned when published. Prior to publishing the   state of a dataset may continue to evolve and the data or metadata are neither   versioned, nor guaranteed to persist. Derivatives of data files may be generated, but original content is   never modified.</li> <li>Replicas: All data files are stored on an AWS public bucket, with replicas   housed at Dartmouth College.  Data files are kept in multiple replicas at the   moment, but this may change over time, and no recovery mechanisms for unversioned data   should be assumed to be in place.</li> <li>Retention period: Versioned items will be retained for the lifetime of the repository.   This is currently the lifetime of the NIH award, which currently expires in   April 2029.</li> <li>Functional preservation: DANDI makes no promises of usability and   understandability of deposited objects.</li> <li>File preservation: Data files and metadata are backed up nightly and   replicated into multiple copies in different storage services.</li> <li>Fixity and authenticity: All data files are stored along with multiple   checksums of the file content. Files are regularly checked against their   checksums to assure that file content remains constant.</li> <li>Succession plans: In case of a repository shutdown, our best efforts will   be made to integrate all content into suitable alternative institutional and/or   other repositories overlapping in the scope of the DANDI archive.</li> </ul> <p>This policy document is derived from the Zenodo General Policies v1.0.</p>"},{"location":"about/terms/","title":"Terms of Use v1.0.1","text":"<p>The DANDI data archive (\"DANDI\") is offered by the DANDI project as part of its mission to make available the results of its work.</p> <p>Use of DANDI, both the uploading and downloading of data, denotes agreement with the following terms:</p> <ol> <li> <p>DANDI is an open dissemination research data repository for the preservation    and making available of research, educational and informational content. Access    to DANDI\u2019s content is open to all.</p> </li> <li> <p>Content may be uploaded free of charge by the US BRAIN Initiative and other    projects required to submit data to a public archive and those without ready    access to an organized data center.</p> </li> <li> <p>The uploader is exclusively responsible for the content that they upload to    DANDI and shall indemnify and hold the DANDI team free and harmless in    connection with their use of the service. The uploader shall ensure that their    content is suitable for open dissemination, and that it complies with these    terms and applicable laws, including, but not limited to, privacy, data    protection and intellectual property rights [1]. In addition, where data that    was originally sensitive personal data is being uploaded for open dissemination    through DANDI, the uploader shall ensure that such data is either anonymized    to an appropriate degree or fully consent cleared [2].</p> </li> <li> <p>Access to DANDI, and all content, is provided on an \"as-is\" basis. Users of    content (\"Users\") shall respect applicable license conditions. Download and    use of content from DANDI does not transfer any intellectual property rights    in the content to the User.</p> </li> <li> <p>Users are exclusively responsible for their use of content, and shall indemnify    and hold the DANDI team free and harmless in connection with their download    and/or use. Hosting and making content available through DANDI does not    represent any approval or endorsement of such content by the DANDI team.</p> </li> <li> <p>The DANDI team reserves the right, without notice, at its sole discretion and    without liability, (i) to alter, delete or block access to content that it    deems to be inappropriate or insufficiently protected, and (ii) to restrict    or remove User access where it considers that use of DANDI interferes with    its operations or violates these Terms of Use or applicable laws.</p> </li> <li> <p>Unless specified otherwise, DANDI metadata may be freely reused under the    CC0 waiver.</p> </li> <li> <p>These Terms of Use are subject to change by the DANDI team at any time and    without notice, other than through posting the updated Terms of Use on the    DANDI website.</p> </li> <li> <p>Uploaders considering DANDI for the storage of unanonymized or encrypted/unencrypted   sensitive personal data are advised to use bespoke platforms rather than open   dissemination services like DANDI for sharing their data.</p> </li> </ol> <p>[1] [2] See further the user pages regarding uploading for information on anonymization of datasets that contain sensitive personal information.</p> <p>If you have any questions or comments with respect to DANDI, or if you are unsure whether your intended use is in line with these Terms of Use, or if you seek permission for a use that does not fall within these Terms of Use, please contact us.</p> <p>This Terms of Service document is derived from the Zenodo terms of service v1.2.</p>"}]}